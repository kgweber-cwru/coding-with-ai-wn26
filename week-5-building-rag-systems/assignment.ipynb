{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kgweber-cwru/coding-with-ai-wn26/blob/main/week-5-building-rag-systems/assignment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Assignment: Build a Domain-Specific RAG System\n",
    "\n",
    "## Objective\n",
    "Build a complete, production-quality RAG system for your domain with proper chunking, metadata, and retrieval strategies.\n",
    "\n",
    "## Requirements\n",
    "1. Use the RAGSystem class (or build your own)\n",
    "2. Process at least 15 documents with metadata\n",
    "3. Implement appropriate chunking\n",
    "4. Test with 5+ realistic questions\n",
    "5. Demonstrate metadata filtering\n",
    "6. Evaluate retrieval quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q google-genai google-auth python-dotenv numpy chromadb\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    try:\n",
    "        PROJECT_ID = input(\"Enter your Google Cloud Project ID (press Enter to use default ADC): \").strip()\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"\"\n",
    "    if PROJECT_ID:\n",
    "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "else:\n",
    "    def find_service_account_json(max_up=6):\n",
    "        p = Path.cwd()\n",
    "        for _ in range(max_up):\n",
    "            candidate = p / \"series-2-coding-llms\" / \"creds\"\n",
    "            if candidate.exists():\n",
    "                for f in candidate.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            candidate2 = p / \"creds\"\n",
    "            if candidate2.exists():\n",
    "                for f in candidate2.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            p = p.parent\n",
    "        return None\n",
    "\n",
    "    sa_path = find_service_account_json()\n",
    "    if sa_path:\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "    else:\n",
    "        try:\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from datetime import datetime\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import google.auth\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "print(f\"Using project: {project}\")\n",
    "\n",
    "print(\"✅ Environment loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy Classes from Concepts Notebook\n",
    "\n",
    "(Include Document, DocumentProcessor, RAGDocumentStore, and RAGSystem classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    \"\"\"Represents a document with metadata\"\"\"\n",
    "    def __init__(self, content, metadata=None):\n",
    "        self.content = content\n",
    "        self.metadata = metadata or {}\n",
    "        self.metadata['created'] = datetime.now().isoformat()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Document(content={self.content[:50]}..., metadata={self.metadata})\"\n",
    "\n",
    "class DocumentProcessor:\n",
    "    \"\"\"Process and chunk documents\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_by_sentences(text, chunk_size=3, overlap=1):\n",
    "        \"\"\"Chunk by sentence count\"\"\"\n",
    "        sentences = [s.strip() + '.' for s in text.split('.') if s.strip()]\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(sentences), chunk_size - overlap):\n",
    "            chunk = ' '.join(sentences[i:i + chunk_size])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    @staticmethod\n",
    "    def chunk_by_words(text, chunk_size=200, overlap=50):\n",
    "        \"\"\"Chunk by word count\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "\n",
    "class ChromaDocumentStore:\n",
    "    \"\"\"Document store using ChromaDB for persistence and efficient search\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name=\"rag_collection\"):\n",
    "        if IN_COLAB:\n",
    "            # Save to Google Drive in Colab\n",
    "            db_path = \"/content/drive/MyDrive/chroma_db\"\n",
    "            if not os.path.exists(db_path):\n",
    "                os.makedirs(db_path)\n",
    "            self.client = chromadb.PersistentClient(path=db_path)\n",
    "        else:\n",
    "            # Save to local disk otherwise\n",
    "            self.client = chromadb.Client()\n",
    "            \n",
    "        self.collection = self.client.get_or_create_collection(name=collection_name)\n",
    "    \n",
    "    def add_document(self, document, chunk_strategy='sentences', chunk_size=3):\n",
    "        \"\"\"Add document with chunking\"\"\"\n",
    "        processor = DocumentProcessor()\n",
    "        \n",
    "        if chunk_strategy == 'sentences':\n",
    "            chunks = processor.chunk_by_sentences(document.content, chunk_size)\n",
    "        else:\n",
    "            chunks = processor.chunk_by_words(document.content, chunk_size)\n",
    "        \n",
    "        if not chunks:\n",
    "            return 0\n",
    "\n",
    "        # Get embeddings for all chunks in one API call\n",
    "        response = client.models.embed_content(\n",
    "            model=\"gemini-embedding-001\",\n",
    "            contents=[c.replace(\"\\n\", \" \") for c in chunks]\n",
    "        )\n",
    "        embeddings = [e.values for e in response.embeddings]\n",
    "        \n",
    "        # Create a unique document ID for tracking\n",
    "        document_id = f\"{document.metadata.get('source', 'doc')}_{datetime.now().timestamp()}\"\n",
    "        \n",
    "        # Prepare metadata and IDs for ChromaDB\n",
    "        metadatas = []\n",
    "        ids = []\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            chunk_metadata = document.metadata.copy()\n",
    "            chunk_metadata['document_id'] = document_id\n",
    "            chunk_metadata['chunk_index'] = i\n",
    "            chunk_metadata['total_chunks'] = len(chunks)\n",
    "            metadatas.append(chunk_metadata)\n",
    "            # Create a unique ID for each chunk\n",
    "            ids.append(f\"{document_id}_{i}\")\n",
    "\n",
    "        # Add to ChromaDB collection\n",
    "        self.collection.add(\n",
    "            embeddings=embeddings,\n",
    "            documents=chunks,\n",
    "            metadatas=metadatas,\n",
    "            ids=ids\n",
    "        )\n",
    "        \n",
    "        return len(chunks)\n",
    "    \n",
    "    def search(self, query, top_k=5, filters=None):\n",
    "        \"\"\"Search with optional metadata filtering\"\"\"\n",
    "        response = client.models.embed_content(\n",
    "            model=\"gemini-embedding-001\",\n",
    "            contents=query\n",
    "        )\n",
    "        query_embedding = response.embeddings[0].values\n",
    "        \n",
    "        # ChromaDB handles filtering with the 'where' clause\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=top_k,\n",
    "            where=filters\n",
    "        )\n",
    "        \n",
    "        # Format results to match our RAG system's expectations\n",
    "        formatted_results = []\n",
    "        if results and results['documents']:\n",
    "            for i, doc in enumerate(results['documents'][0]):\n",
    "                formatted_results.append({\n",
    "                    \"chunk\": doc,\n",
    "                    \"similarity\": 1 - results['distances'][0][i], # Chroma uses distance, convert to similarity\n",
    "                    \"metadata\": results['metadatas'][0][i]\n",
    "                })\n",
    "        \n",
    "        return formatted_results\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.collection.count()\n",
    "\n",
    "class RAGSystem:\n",
    "    \"\"\"Complete RAG system with a pluggable document store\"\"\"\n",
    "    \n",
    "    def __init__(self, store_impl=ChromaDocumentStore):\n",
    "        self.store = store_impl()\n",
    "    \n",
    "    def add_document(self, content, metadata=None, **kwargs):\n",
    "        \"\"\"Add document to system\"\"\"\n",
    "        doc = Document(content, metadata)\n",
    "        chunks_added = self.store.add_document(doc, **kwargs)\n",
    "        return chunks_added\n",
    "    \n",
    "    def query(self, question, top_k=3, filters=None, temperature=0.3):\n",
    "        \"\"\"Query system with RAG\"\"\"\n",
    "        # Retrieve\n",
    "        results = self.store.search(question, top_k=top_k, filters=filters)\n",
    "        \n",
    "        if not results:\n",
    "            return {\n",
    "                \"answer\": \"No relevant documents found.\",\n",
    "                \"sources\": []\n",
    "            }\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[Source {i+1}] {r['chunk']}\"\n",
    "            for i, r in enumerate(results)\n",
    "        ])\n",
    "        \n",
    "        # Generate\n",
    "        prompt = f\"\"\"Answer the question based on the provided context. \n",
    "If the answer is not in the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-2.5-flash-lite\",\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(temperature=temperature)\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response.text,\n",
    "            \"sources\": results\n",
    "        }\n",
    "    \n",
    "    def stats(self):\n",
    "        \"\"\"Get system statistics\"\"\"\n",
    "        total_chunks = len(self.store)\n",
    "        # To get unique docs, count unique document_id values in metadata\n",
    "        if isinstance(self.store, ChromaDocumentStore):\n",
    "            all_meta = self.store.collection.get(include=['metadatas'])\n",
    "            unique_docs = len(set(m.get('document_id', '') for m in all_meta['metadatas']))\n",
    "        else:\n",
    "            unique_docs = \"N/A\"\n",
    "\n",
    "        return {\n",
    "            \"total_chunks\": total_chunks,\n",
    "            \"unique_documents\": unique_docs,\n",
    "            \"store_implementation\": self.store.__class__.__name__\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"✅ RAG system classes ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Describe Your System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR DESCRIPTION**\n",
    "\n",
    "Domain: [your field]\n",
    "\n",
    "Use case: [what problem does this solve?]\n",
    "\n",
    "Document types: [what you're indexing]\n",
    "\n",
    "Target users: [who will use this?]\n",
    "\n",
    "Metadata strategy: [what metadata will you track and why?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Your Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your document collection with metadata\n",
    "documents = [\n",
    "    {\n",
    "        \"content\": \"\"\"YOUR DOCUMENT CONTENT\"\"\",\n",
    "        \"metadata\": {\n",
    "            \"source\": \"...\",\n",
    "            \"category\": \"...\",\n",
    "            # Add relevant metadata\n",
    "        }\n",
    "    },\n",
    "    # Add at least 15 documents\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Your RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize system\n",
    "rag = RAGSystem()\n",
    "\n",
    "# Add documents\n",
    "print(\"Building knowledge base...\")\n",
    "for doc in documents:\n",
    "    chunks = rag.add_document(\n",
    "        doc[\"content\"],\n",
    "        doc[\"metadata\"],\n",
    "        chunk_strategy='sentences',  # or 'words'\n",
    "        chunk_size=3  # adjust as needed\n",
    "    )\n",
    "    print(f\"  Added document: {chunks} chunks\")\n",
    "\n",
    "print(f\"\\nSystem ready! Stats: {rag.stats()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test with Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"YOUR QUESTION 1\",\n",
    "    \"YOUR QUESTION 2\",\n",
    "    \"YOUR QUESTION 3\",\n",
    "    \"YOUR QUESTION 4\",\n",
    "    \"YOUR QUESTION 5\",\n",
    "]\n",
    "\n",
    "for q in test_questions:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Q: {q}\")\n",
    "    print('='*70)\n",
    "    \n",
    "    result = rag.query(q, top_k=3)\n",
    "    \n",
    "    print(f\"\\nA: {result['answer']}\")\n",
    "    print(\"\\nSources:\")\n",
    "    for i, src in enumerate(result['sources'], 1):\n",
    "        print(f\"  {i}. [Score: {src['similarity']:.3f}]\")\n",
    "        print(f\"     {src['chunk'][:100]}...\")\n",
    "        print(f\"     Metadata: {src['metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Demonstrate Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test metadata filtering\n",
    "filtered_question = \"YOUR QUESTION\"\n",
    "filter_criteria = {\"YOUR_METADATA_KEY\": \"YOUR_VALUE\"}\n",
    "\n",
    "print(f\"Question: {filtered_question}\")\n",
    "print(f\"Filter: {filter_criteria}\\n\")\n",
    "\n",
    "result = rag.query(filtered_question, filters=filter_criteria, top_k=3)\n",
    "print(f\"Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Evaluate Retrieval Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually evaluate: for each question, were the right chunks retrieved?\n",
    "\n",
    "evaluation = [\n",
    "    {\n",
    "        \"question\": \"YOUR QUESTION\",\n",
    "        \"relevant_chunks_found\": True/False,\n",
    "        \"answer_quality\": \"good/fair/poor\",\n",
    "        \"notes\": \"YOUR OBSERVATIONS\"\n",
    "    },\n",
    "    # Evaluate each test question\n",
    "]\n",
    "\n",
    "# Calculate metrics\n",
    "success_rate = sum(1 for e in evaluation if e['relevant_chunks_found']) / len(evaluation)\n",
    "print(f\"Retrieval success rate: {success_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "### 1. How did chunking affect retrieval quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Was metadata filtering useful? When would you use it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What improvements would make this production-ready?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What challenges did you encounter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Save/Load System\n",
    "\n",
    "Implement persistence so your system can be saved and loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
