{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install -q google-genai google-auth python-dotenv\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    try:\n",
    "        PROJECT_ID = input(\"Enter your Google Cloud Project ID (press Enter to use default ADC): \").strip()\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"\"\n",
    "    if PROJECT_ID:\n",
    "        import os\n",
    "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "\n",
    "import os\n",
    "import google.auth\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "print(f\"Using project: {project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kgweber-cwru/coding-with-ai-wn26/blob/main/series-2-coding-llms/week-3-prompt-engineering/concepts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3: Programmatic Prompt Engineering\n",
    "\n",
    "## Learning Objectives\n",
    "- Build dynamic prompts using templates\n",
    "- Implement few-shot learning programmatically\n",
    "- Parse and validate LLM outputs\n",
    "- Create reusable prompt libraries\n",
    "- Handle structured data in prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import google.auth\n",
    "\n",
    "load_dotenv()\n",
    "creds, project = google.auth.default()\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "\n",
    "print(\"âœ“ Ready for prompt engineering!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Dynamic Prompt Templates\n",
    "\n",
    "### Basic Template with f-strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(article_text, max_sentences=3, style=\"academic\"):\n",
    "    \"\"\"Summarize an article with customizable parameters\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Summarize the following article in {max_sentences} sentences.\n",
    "    Use a {style} writing style.\n",
    "    \n",
    "    Article:\n",
    "    {article_text}\n",
    "    \n",
    "    Summary:\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0.3)\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "# Test it\n",
    "sample_article = \"\"\"Machine learning has transformed healthcare diagnostics. \n",
    "Recent studies show AI models can detect diseases from medical imaging with high accuracy. \n",
    "However, challenges remain in model interpretability and clinical integration.\"\"\"\n",
    "\n",
    "print(\"Academic style:\")\n",
    "print(summarize_article(sample_article, 2, \"academic\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Plain language:\")\n",
    "print(summarize_article(sample_article, 1, \"plain language\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Template Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptTemplate:\n",
    "    \"\"\"Reusable prompt template with validation\"\"\"\n",
    "    \n",
    "    def __init__(self, template, required_vars=None):\n",
    "        self.template = template\n",
    "        self.required_vars = required_vars or []\n",
    "    \n",
    "    def format(self, **kwargs):\n",
    "        \"\"\"Format template with variables, checking all required vars provided\"\"\"\n",
    "        missing = [var for var in self.required_vars if var not in kwargs]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required variables: {missing}\")\n",
    "        \n",
    "        return self.template.format(**kwargs)\n",
    "    \n",
    "    def run(self, **kwargs):\n",
    "        \"\"\"Format and execute the prompt\"\"\"\n",
    "        prompt = self.format(**kwargs)\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=kwargs.get('temperature', 0.3)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return response.text\n",
    "\n",
    "# Create a template\n",
    "email_template = PromptTemplate(\n",
    "    template=\"\"\"Write a professional email with the following details:\n",
    "    \n",
    "    To: {recipient}\n",
    "    Subject: {subject}\n",
    "    Tone: {tone}\n",
    "    Key points to include:\n",
    "    {key_points}\n",
    "    \n",
    "    Email:\"\"\",\n",
    "    required_vars=['recipient', 'subject', 'tone', 'key_points']\n",
    ")\n",
    "\n",
    "# Use it\n",
    "email = email_template.run(\n",
    "    recipient=\"Dr. Smith\",\n",
    "    subject=\"Research Collaboration Proposal\",\n",
    "    tone=\"friendly but professional\",\n",
    "    key_points=\"- Interested in collaborating on AI project\\n- Have preliminary data\\n- Request meeting next week\"\n",
    ")\n",
    "\n",
    "print(email)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Few-Shot Learning\n",
    "\n",
    "Provide examples to guide the model's behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotClassifier:\n",
    "    \"\"\"Classify text using few-shot examples\"\"\"\n",
    "    \n",
    "    def __init__(self, examples, task_description):\n",
    "        self.examples = examples  # List of (input, output) tuples\n",
    "        self.task_description = task_description\n",
    "    \n",
    "    def build_prompt(self, new_input):\n",
    "        \"\"\"Build prompt with examples\"\"\"\n",
    "        prompt = f\"{self.task_description}\\n\\n\"\n",
    "        \n",
    "        # Add examples\n",
    "        for inp, out in self.examples:\n",
    "            prompt += f\"Input: {inp}\\nOutput: {out}\\n\\n\"\n",
    "        \n",
    "        # Add new input\n",
    "        prompt += f\"Input: {new_input}\\nOutput:\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def classify(self, text):\n",
    "        prompt = self.build_prompt(text)\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            contents=prompt,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=0,\n",
    "                max_output_tokens=50\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return response.text.strip()\n",
    "\n",
    "# Example: Medical note classification\n",
    "medical_classifier = FewShotClassifier(\n",
    "    examples=[\n",
    "        (\"Patient reports chest pain and shortness of breath\", \"URGENT\"),\n",
    "        (\"Routine follow-up appointment scheduled\", \"ROUTINE\"),\n",
    "        (\"Lab results show elevated blood sugar\", \"FOLLOW-UP\"),\n",
    "    ],\n",
    "    task_description=\"Classify medical notes as: URGENT, ROUTINE, or FOLLOW-UP\"\n",
    ")\n",
    "\n",
    "# Test it\n",
    "test_cases = [\n",
    "    \"Patient has persistent fever and confusion\",\n",
    "    \"Annual physical examination completed\",\n",
    "    \"X-ray shows possible fracture\"\n",
    "]\n",
    "\n",
    "for case in test_cases:\n",
    "    classification = medical_classifier.classify(case)\n",
    "    print(f\"Note: {case}\")\n",
    "    print(f\"Classification: {classification}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Structured Output Parsing\n",
    "\n",
    "### JSON Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_structured_data(text, schema):\n",
    "    \"\"\"Extract structured data according to schema\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Extract information from the text according to this schema.\n",
    "    \n",
    "    Schema: {json.dumps(schema, indent=2)}\n",
    "    \n",
    "    Text: {text}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0,\n",
    "            response_mime_type=\"application/json\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Parse JSON\n",
    "    try:\n",
    "        return json.loads(response.text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Fallback if needed\n",
    "        content = response.text\n",
    "        start = content.find('{')\n",
    "        end = content.rfind('}') + 1\n",
    "        return json.loads(content[start:end])\n",
    "\n",
    "# Example: Extract patient information\n",
    "patient_text = \"\"\"John Smith, age 45, presented with hypertension. \n",
    "Blood pressure: 150/95. Prescribed lisinopril 10mg daily. \n",
    "Follow-up in 2 weeks.\"\"\"\n",
    "\n",
    "schema = {\n",
    "    \"patient_name\": \"string\",\n",
    "    \"age\": \"integer\",\n",
    "    \"diagnosis\": \"string\",\n",
    "    \"medication\": \"string\",\n",
    "    \"follow_up\": \"string\"\n",
    "}\n",
    "\n",
    "result = extract_structured_data(patient_text, schema)\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation and Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_with_validation(text, schema, max_retries=2):\n",
    "    \"\"\"Extract data with retry on validation failure\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            result = extract_structured_data(text, schema)\n",
    "            \n",
    "            # Validate all required fields present\n",
    "            missing = [key for key in schema.keys() if key not in result]\n",
    "            if missing:\n",
    "                if attempt < max_retries:\n",
    "                    print(f\"Attempt {attempt + 1}: Missing fields {missing}, retrying...\")\n",
    "                    continue\n",
    "                else:\n",
    "                    raise ValueError(f\"Missing required fields: {missing}\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries:\n",
    "                print(f\"Attempt {attempt + 1} failed: {e}, retrying...\")\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Test it\n",
    "result = extract_with_validation(patient_text, schema)\n",
    "print(\"Validated result:\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Batch Processing with Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_process(items, template_func, **kwargs):\n",
    "    \"\"\"Process multiple items with same template\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for i, item in enumerate(items, 1):\n",
    "        print(f\"Processing {i}/{len(items)}...\", end=\" \")\n",
    "        result = template_func(item, **kwargs)\n",
    "        results.append({\"input\": item, \"output\": result})\n",
    "        print(\"Done\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def translate_to_plain_language(medical_term):\n",
    "    \"\"\"Translate medical jargon to plain language\"\"\"\n",
    "    prompt = f\"\"\"Explain this medical term in simple, patient-friendly language (one sentence):\n",
    "    \n",
    "    Term: {medical_term}\n",
    "    \n",
    "    Plain language explanation:\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=0.3,\n",
    "            max_output_tokens=100\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "# Process multiple terms\n",
    "medical_terms = [\n",
    "    \"hypertension\",\n",
    "    \"myocardial infarction\",\n",
    "    \"dyspnea\"\n",
    "]\n",
    "\n",
    "translations = batch_process(medical_terms, translate_to_plain_language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Prompt Library Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptLibrary:\n",
    "    \"\"\"Manage a collection of reusable prompts\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.prompts = {}\n",
    "    \n",
    "    def register(self, name, template, required_vars=None):\n",
    "        \"\"\"Register a new prompt template\"\"\"\n",
    "        self.prompts[name] = PromptTemplate(template, required_vars)\n",
    "    \n",
    "    def get(self, name):\n",
    "        \"\"\"Get a prompt template by name\"\"\"\n",
    "        if name not in self.prompts:\n",
    "            raise ValueError(f\"Prompt '{name}' not found\")\n",
    "        return self.prompts[name]\n",
    "    \n",
    "    def list(self):\n",
    "        \"\"\"List all registered prompts\"\"\"\n",
    "        return list(self.prompts.keys())\n",
    "\n",
    "# Create library\n",
    "lib = PromptLibrary()\n",
    "\n",
    "# Register prompts\n",
    "lib.register(\n",
    "    \"summarize\",\n",
    "    \"Summarize this text in {num_sentences} sentences:\\n\\n{text}\\n\\nSummary:\",\n",
    "    required_vars=['text', 'num_sentences']\n",
    ")\n",
    "\n",
    "lib.register(\n",
    "    \"expand\",\n",
    "    \"Expand this concept with {detail_level} detail:\\n\\n{concept}\\n\\nExpanded explanation:\",\n",
    "    required_vars=['concept', 'detail_level']\n",
    ")\n",
    "\n",
    "lib.register(\n",
    "    \"compare\",\n",
    "    \"Compare and contrast {item_a} and {item_b}. Focus on: {focus}\\n\\nComparison:\",\n",
    "    required_vars=['item_a', 'item_b', 'focus']\n",
    ")\n",
    "\n",
    "# Use prompts\n",
    "print(\"Available prompts:\", lib.list())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "result = lib.get('compare').run(\n",
    "    item_a=\"MRI\",\n",
    "    item_b=\"CT scan\",\n",
    "    focus=\"clinical applications and safety considerations\"\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Templates enable reusability** - Write once, use many times\n",
    "2. **Few-shot learning is powerful** - Good examples guide behavior\n",
    "3. **Structure your outputs** - JSON makes parsing reliable\n",
    "4. **Validate and retry** - LLMs aren't perfect, build in checks\n",
    "5. **Build libraries** - Organize prompts for team use\n",
    "\n",
    "## Next Week\n",
    "\n",
    "We'll explore **embeddings and RAG concepts**:\n",
    "- Understanding vector representations\n",
    "- Semantic similarity\n",
    "- Document retrieval\n",
    "- Setting up for RAG systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
