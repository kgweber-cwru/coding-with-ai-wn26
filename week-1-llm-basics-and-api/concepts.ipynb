{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q google-genai google-auth python-dotenv numpy\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    try:\n",
    "        PROJECT_ID = input(\"Enter your Google Cloud Project ID (press Enter to use default ADC): \").strip()\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"\"\n",
    "    if PROJECT_ID:\n",
    "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "else:\n",
    "    def find_service_account_json(max_up=6):\n",
    "        p = Path.cwd()\n",
    "        for _ in range(max_up):\n",
    "            candidate = p / \"series-2-coding-llms\" / \"creds\"\n",
    "            if candidate.exists():\n",
    "                for f in candidate.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            candidate2 = p / \"creds\"\n",
    "            if candidate2.exists():\n",
    "                for f in candidate2.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            p = p.parent\n",
    "        return None\n",
    "\n",
    "    sa_path = find_service_account_json()\n",
    "    if sa_path:\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "    else:\n",
    "        try:\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kgweber-cwru/coding-with-ai-wn26/blob/main/week-1-llm-basics-and-api/concepts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Understanding LLMs and API Basics\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- Understand how large language models work at a conceptual level\n",
    "- Successfully make API calls to Vertex AI\n",
    "- Understand key API parameters and their effects\n",
    "- Build simple text generation scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: How LLMs Work (Conceptual Overview)\n",
    "\n",
    "### Tokens: The Building Blocks\n",
    "- LLMs don't see words, they see **tokens**\n",
    "- A token is roughly 3-4 characters or about 0.75 words\n",
    "- \"Hello world\" ≈ 2-3 tokens\n",
    "- This matters for cost and context limits!\n",
    "\n",
    "### Prediction and Probability\n",
    "- LLMs predict the next token based on all previous tokens\n",
    "- They assign probabilities to many possible next tokens\n",
    "- They don't \"know\" things - they predict statistically likely continuations\n",
    "- Temperature controls randomness in selection\n",
    "\n",
    "### Key Limitations\n",
    "- No real-time information (knowledge cutoff dates)\n",
    "- Can \"hallucinate\" plausible-sounding but false information\n",
    "- Cannot count tokens or characters perfectly\n",
    "- Context window limits (how much text they can \"remember\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting Up Your Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import google.auth\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "print(f\"Using project: {project}\")\n",
    "\n",
    "print(\"✓ Environment loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Your First API Call\n",
    "\n",
    "The basic structure of a Vertex AI API call:\n",
    "```python\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Your prompt here\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple completion\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"Say hello!\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response Object\n",
    "\n",
    "Let's examine what the API returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make another call and explore the response\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"What is 2+2?\"\n",
    ")\n",
    "\n",
    "print(\"Full response object:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Just the content:\")\n",
    "print(response.text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Token usage:\")\n",
    "# Check if usage metadata is available\n",
    "if response.usage_metadata:\n",
    "    print(f\"Prompt tokens: {response.usage_metadata.prompt_token_count}\")\n",
    "    print(f\"Completion tokens: {response.usage_metadata.candidates_token_count}\")\n",
    "    print(f\"Total tokens: {response.usage_metadata.total_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Key API Parameters\n",
    "\n",
    "### Temperature (0.0 to 2.0)\n",
    "Controls randomness:\n",
    "- **0.0**: Deterministic, always picks most likely token\n",
    "- **0.7**: Balanced (default for most uses)\n",
    "- **1.5+**: Very creative/random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.0:\n",
      "Here are a few ways to complete that sentence, depending on the emphasis you want to make:\n",
      "\n",
      "**Focusing on the creative aspect:**\n",
      "\n",
      "* The best thing about learning to code is **the ability to bring your ideas to life and build something\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Temperature 0.7:\n",
      "Here are several ways to complete that sentence, depending on the emphasis you want to place:\n",
      "\n",
      "**Focusing on creation and problem-solving:**\n",
      "\n",
      "* The best thing about learning to code is **the power to create anything you can imagine.**\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Temperature 1.5:\n",
      "Here are a few ways to complete that sentence, each highlighting a different aspect of why coding is so rewarding:\n",
      "\n",
      "**Focusing on creativity and building:**\n",
      "\n",
      "* The best thing about learning to code is **the power to build anything you can imagine\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's compare different temperatures\n",
    "prompt = \"Complete this sentence: The best thing about learning to code is\"\n",
    "\n",
    "for temp in [0.0, 0.7, 1.5]:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=temp,\n",
    "            max_output_tokens=50\n",
    "        )\n",
    "    )\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(response.text)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Tokens\n",
    "Limits the length of the response. Important for cost control!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens: 20\n",
      "Let's break down what a **Large Language Model (LLM)** is in a clear and\n",
      "Actual tokens used: 20\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Max tokens: 50\n",
      "Let's break down what a **Large Language Model (LLM)** is in a clear and understandable way.\n",
      "\n",
      "Imagine you're trying to teach a computer to understand and generate human language. This is a monumental task, as language is incredibly\n",
      "Actual tokens used: 50\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Max tokens: 150\n",
      "Let's break down what a **Large Language Model (LLM)** is in a way that's easy to understand.\n",
      "\n",
      "Imagine you have a **super-powered, incredibly well-read digital assistant** that's been fed a massive library of text and code from the internet. This assistant doesn't just read; it learns the *patterns*, the *relationships*, and the *nuances* of language.\n",
      "\n",
      "Here's a more structured explanation:\n",
      "\n",
      "**At its core, a Large Language Model is a type of Artificial Intelligence (AI) designed to understand, generate, and process human language.**\n",
      "\n",
      "Let's unpack the key components:\n",
      "\n",
      "1.  **\"Large\":** This refers to two main things:\n",
      "Actual tokens used: 150\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare different max_output_tokens\n",
    "prompt = \"Explain what a large language model is.\"\n",
    "\n",
    "for max_tok in [20, 50, 150]:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            max_output_tokens=max_tok\n",
    "        )\n",
    "    )\n",
    "    print(f\"Max tokens: {max_tok}\")\n",
    "    print(response.text)\n",
    "    if response.usage_metadata:\n",
    "        print(f\"Actual tokens used: {response.usage_metadata.candidates_token_count}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Instructions\n",
    "Set the behavior and personality of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without system message:\n",
      "DNA, or **deoxyribonucleic acid**, is a complex molecule that carries the genetic instructions for the development, functioning, growth, and reproduction of all known living organisms and many viruses. Think of it as the **blueprint** or **instruction manual** for life.\n",
      "\n",
      "Here's a breakdown of what DNA is and its key characteristics:\n",
      "\n",
      "**1. Structure:**\n",
      "\n",
      "*   **Double Helix:** The most famous feature of DNA is its double helix structure, resembling a twisted ladder. This structure was first accurately described by James Watson and Francis Crick, with crucial contributions from Rosalind Franklin.\n",
      "*   **Nucleotides:** Each strand of the DNA ladder is made up of repeating units called **nucleotides**.\n",
      "*   **Components of a Nucleotide:** Each nucleotide has three parts:\n",
      "    *   **A Phosphate Group:** This is the \"backbone\" of the DNA strand.\n",
      "    *   **A Deoxyribose Sugar:** Another component of the backbone.\n",
      "    *   **A Nitrogenous Base:** This is the part that carries the genetic information. There are four types of nitrogenous bases in DNA:\n",
      "        *   **Adenine (A)**\n",
      "        *   **Guanine (G)**\n",
      "        *   **Cytosine (C)**\n",
      "        *   **Thymine (T)**\n",
      "\n",
      "*   **Base Pairing:** The two strands of the DNA helix are held together by specific pairings between the nitrogenous bases:\n",
      "    *   **Adenine (A) always pairs with Thymine (T).**\n",
      "    *   **Guanine (G) always pairs with Cytosine (C).**\n",
      "    *   These pairings are known as **complementary base pairing** and are crucial for DNA replication and the accurate transmission of genetic information.\n",
      "\n",
      "**2. Function:**\n",
      "\n",
      "*   **Carries Genetic Information:** The sequence of these bases (A, T, C, G) along the DNA molecule forms a code. This code is read in units called **genes**.\n",
      "*   **Genes Code for Proteins:** Genes are essentially instructions for building specific proteins. Proteins are the workhorses of the cell, performing a vast array of functions, from catalyzing chemical reactions (enzymes) to providing structural support and transporting molecules.\n",
      "*   **Heredity:** DNA is passed down from parents to offspring, ensuring the inheritance of traits. This is why children resemble their parents.\n",
      "*   **Cellular Function:** DNA directs all cellular activities, including metabolism, growth, and division.\n",
      "\n",
      "**3. Location:**\n",
      "\n",
      "*   **In Eukaryotes (like humans, animals, plants):** DNA is primarily found in the **nucleus** of the cell, organized into structures called **chromosomes**. A small amount of DNA is also found in the **mitochondria** (often called the \"powerhouses\" of the cell).\n",
      "*   **In Prokaryotes (like bacteria):** DNA is typically found in a region of the cytoplasm called the **nucleoid**, and it's usually in a circular form.\n",
      "\n",
      "**In summary, DNA is the fundamental molecule of heredity and life. Its unique double helix structure and the specific sequence of its bases encode all the instructions necessary for an organism to develop, survive, and reproduce.**\n",
      "\n",
      "Understanding DNA is at the heart of many scientific fields, including genetics, molecular biology, medicine, and biotechnology.\n",
      "\n",
      "==================================================\n",
      "\n",
      "With system message (10-year-old level):\n",
      "Imagine your body is like a giant instruction manual, but instead of telling you how to build a toy or bake a cake, it tells your body how to grow, what color your eyes should be, and even how tall you might get!\n",
      "\n",
      "That amazing instruction manual is called **DNA**.\n",
      "\n",
      "Think of DNA as a super long, twisted ladder. This ladder has lots of little rungs, and these rungs are like letters. The order of these letters spells out all the instructions for you!\n",
      "\n",
      "Here's a fun way to think about it:\n",
      "\n",
      "*   **The Ladder:** This is the basic shape of DNA, called a **double helix**. It's like a spiral staircase you'd see in a castle!\n",
      "*   **The Rungs:** These are made of tiny building blocks. There are only four different kinds of these blocks, and we call them **bases**. Think of them like four different colored LEGO bricks:\n",
      "    *   **A** (like a red brick)\n",
      "    *   **T** (like a blue brick)\n",
      "    *   **C** (like a yellow brick)\n",
      "    *   **G** (like a green brick)\n",
      "\n",
      "These four \"letters\" (A, T, C, G) can be arranged in *trillions* of different ways to make up all the instructions for you.\n",
      "\n",
      "**Where is this DNA?**\n",
      "\n",
      "It's inside almost every tiny little room (called a **cell**) that makes up your whole body! It's like each room has its own copy of the instruction manual.\n",
      "\n",
      "**What does DNA do?**\n",
      "\n",
      "It tells your body:\n",
      "*   What color hair you have.\n",
      "*   Whether you're likely to have freckles.\n",
      "*   How your heart should beat.\n",
      "*   And SO much more!\n",
      "\n",
      "It's like the blueprint for making *you*! And guess what? You get half of your DNA instruction manual from your mom and the other half from your dad. That's why you might look a little like your mom and a little like your dad!\n",
      "\n",
      "So, next time you wonder why you have a certain trait, remember it's all thanks to the amazing, twisted ladder of **DNA** inside you! Pretty cool, right?\n"
     ]
    }
   ],
   "source": [
    "# Without system message\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"What is DNA?\"\n",
    ")\n",
    "print(\"Without system message:\")\n",
    "print(response.text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# With system instruction\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"What is DNA?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"You are a biology teacher explaining concepts to 10-year-olds. Use simple language and fun analogies.\"\n",
    "    )\n",
    ")\n",
    "print(\"With system message (10-year-old level):\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Practical Examples\n",
    "\n",
    "### Example 1: Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Large language models are AI systems trained on massive text datasets that learn language patterns to generate human-like text and perform various language-related tasks.\n"
     ]
    }
   ],
   "source": [
    "long_text = \"\"\"\n",
    "Large language models are artificial intelligence systems trained on vast amounts of text data. \n",
    "They learn patterns in language by predicting the next word in a sequence. These models have billions \n",
    "of parameters and can generate human-like text, answer questions, write code, and perform various \n",
    "language tasks. They work by converting text into numerical representations called tokens, processing \n",
    "these tokens through neural network layers, and generating probability distributions for likely next tokens.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=long_text,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"Summarize the following text in one sentence.\",\n",
    "        temperature=0.3\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this new feature!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: This is the worst experience ever.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: The product arrived on time.\n",
      "Sentiment: Neutral\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def classify_sentiment(text):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=text,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=\"Classify the sentiment of the text as: positive, negative, or neutral. Respond with only one word.\",\n",
    "            temperature=0,\n",
    "            max_output_tokens=10\n",
    "        )\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "# Test it\n",
    "test_texts = [\n",
    "    \"I love this new feature!\",\n",
    "    \"This is the worst experience ever.\",\n",
    "    \"The product arrived on time.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    sentiment = classify_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted information:\n",
      "Name: John Smith | Email: john.smith@email.com | Phone: 555-123-4567\n"
     ]
    }
   ],
   "source": [
    "def extract_info(text):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=text,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=\"Extract the person's name, email, and phone number from the text. Format as: Name: X | Email: Y | Phone: Z\",\n",
    "            temperature=0\n",
    "        )\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "contact_text = \"Hi, I'm John Smith. You can reach me at john.smith@email.com or call me at 555-123-4567.\"\n",
    "\n",
    "extracted = extract_info(contact_text)\n",
    "print(\"Extracted information:\")\n",
    "print(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Cost Awareness\n",
    "\n",
    "Understanding and tracking your API costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cursor awaits,\n",
      "Logic flows from mind to screen,\n",
      "A new world is born.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Tokens used: 2046\n",
      "Estimated cost: $0.00000830 (or 0.000830 cents)\n"
     ]
    }
   ],
   "source": [
    "# Pricing \n",
    "# Gemini 2.5 Flash Lite ~$0.10 per 1M input tokens, 0.40 per 1M output\n",
    "\n",
    "def estimate_cost(response, model=\"gemini-2.5-flash-lite\"):\n",
    "    \"\"\"Estimate the cost of an API call\"\"\"\n",
    "    # Example pricing (verify at cloud.google.com/vertex-ai/pricing)\n",
    "    pricing = {\n",
    "        \"gemini-2.5-flash-lite\": {\"input\": 0.10, \"output\": 0.40}, \n",
    "        \"gemini-2.5-pro\": {\"input\": 1.25, \"output\": 10}\n",
    "    }\n",
    "    \n",
    "    # Handle unknown models or default\n",
    "    if model not in pricing:\n",
    "        model = \"gemini-2.5-flash-lite\"\n",
    "        \n",
    "    if not response.usage_metadata:\n",
    "        return {\"cost_usd\": 0, \"total_tokens\": 0}\n",
    "\n",
    "    input_tokens = response.usage_metadata.prompt_token_count\n",
    "    output_tokens = response.usage_metadata.candidates_token_count\n",
    "    total_tokens = response.usage_metadata.total_token_count\n",
    "\n",
    "    input_cost = (input_tokens / 1_000_000) * pricing[model][\"input\"]\n",
    "    output_cost = (output_tokens / 1_000_000) * pricing[model][\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"cost_usd\": total_cost,\n",
    "        \"cost_cents\": total_cost * 100\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"Write a haiku about programming.\"\n",
    ")\n",
    "\n",
    "print(response.text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "cost_info = estimate_cost(response, model=\"gemini-2.5-flash-lite\")\n",
    "print(f\"\\nTokens used: {cost_info['total_tokens']}\")\n",
    "print(f\"Estimated cost: ${cost_info['cost_usd']:.8f} (or {cost_info['cost_cents']:.6f} cents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **LLMs predict tokens** based on probability, they don't \"know\" facts\n",
    "2. **API structure** is simple: model + contents + parameters\n",
    "3. **Temperature** controls randomness (0 = deterministic, higher = creative)\n",
    "4. **max_output_tokens** limits response length and controls costs\n",
    "5. **System instructions** shape the model's behavior\n",
    "6. **Always monitor costs** - even small calls add up!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Week 2, we'll learn how to:\n",
    "- Maintain conversation history\n",
    "- Build multi-turn conversations\n",
    "- Manage context effectively\n",
    "\n",
    "Complete the assignment to practice these concepts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
