{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Understanding LLMs and API Basics\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- Understand how large language models work at a conceptual level\n",
    "- Successfully make API calls to OpenAI\n",
    "- Understand key API parameters and their effects\n",
    "- Build simple text generation scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: How LLMs Work (Conceptual Overview)\n",
    "\n",
    "### Tokens: The Building Blocks\n",
    "- LLMs don't see words, they see **tokens**\n",
    "- A token is roughly 3-4 characters or about 0.75 words\n",
    "- \"Hello world\" ≈ 2-3 tokens\n",
    "- This matters for cost and context limits!\n",
    "\n",
    "### Prediction and Probability\n",
    "- LLMs predict the next token based on all previous tokens\n",
    "- They assign probabilities to many possible next tokens\n",
    "- They don't \"know\" things - they predict statistically likely continuations\n",
    "- Temperature controls randomness in selection\n",
    "\n",
    "### Key Limitations\n",
    "- No real-time information (knowledge cutoff dates)\n",
    "- Can \"hallucinate\" plausible-sounding but false information\n",
    "- Cannot count tokens or characters perfectly\n",
    "- Context window limits (how much text they can \"remember\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting Up Your Environment\n",
    "\n",
    "### Load Environment Variables\n",
    "We use `python-dotenv` to keep API keys secure and separate from code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "print(\"✓ Environment loaded successfully!\")\n",
    "print(f\"✓ API key found: {os.getenv('OPENAI_API_KEY')[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Your First API Call\n",
    "\n",
    "The basic structure of an OpenAI API call:\n",
    "```python\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Your prompt here\"}]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Say hello!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response Object\n",
    "\n",
    "Let's examine what the API returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make another call and explore the response\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Full response object:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Just the content:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Token usage:\")\n",
    "print(f\"Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "print(f\"Completion tokens: {response.usage.completion_tokens}\")\n",
    "print(f\"Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Key API Parameters\n",
    "\n",
    "### Temperature (0.0 to 2.0)\n",
    "Controls randomness:\n",
    "- **0.0**: Deterministic, always picks most likely token\n",
    "- **0.7**: Balanced (default for most uses)\n",
    "- **1.5+**: Very creative/random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare different temperatures\n",
    "prompt = \"Complete this sentence: The best thing about learning to code is\"\n",
    "\n",
    "for temp in [0.0, 0.7, 1.5]:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=temp,\n",
    "        max_tokens=50\n",
    "    )\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Tokens\n",
    "Limits the length of the response. Important for cost control!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different max_tokens\n",
    "prompt = \"Explain what a large language model is.\"\n",
    "\n",
    "for max_tok in [20, 50, 150]:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tok\n",
    "    )\n",
    "    print(f\"Max tokens: {max_tok}\")\n",
    "    print(response.choices[0].message.content)\n",
    "    print(f\"Actual tokens used: {response.usage.completion_tokens}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Messages\n",
    "Set the behavior and personality of the assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without system message\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is DNA?\"}\n",
    "    ]\n",
    ")\n",
    "print(\"Without system message:\")\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# With system message\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a biology teacher explaining concepts to 10-year-olds. Use simple language and fun analogies.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is DNA?\"}\n",
    "    ]\n",
    ")\n",
    "print(\"With system message (10-year-old level):\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Practical Examples\n",
    "\n",
    "### Example 1: Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"\"\"\n",
    "Large language models are artificial intelligence systems trained on vast amounts of text data. \n",
    "They learn patterns in language by predicting the next word in a sequence. These models have billions \n",
    "of parameters and can generate human-like text, answer questions, write code, and perform various \n",
    "language tasks. They work by converting text into numerical representations called tokens, processing \n",
    "these tokens through neural network layers, and generating probability distributions for likely next tokens.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Summarize the following text in one sentence.\"},\n",
    "        {\"role\": \"user\", \"content\": long_text}\n",
    "    ],\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Classify the sentiment of the text as: positive, negative, or neutral. Respond with only one word.\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        temperature=0,\n",
    "        max_tokens=10\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# Test it\n",
    "test_texts = [\n",
    "    \"I love this new feature!\",\n",
    "    \"This is the worst experience ever.\",\n",
    "    \"The product arrived on time.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    sentiment = classify_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(text):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Extract the person's name, email, and phone number from the text. Format as: Name: X | Email: Y | Phone: Z\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "contact_text = \"Hi, I'm John Smith. You can reach me at john.smith@email.com or call me at 555-123-4567.\"\n",
    "\n",
    "extracted = extract_info(contact_text)\n",
    "print(\"Extracted information:\")\n",
    "print(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Cost Awareness\n",
    "\n",
    "Understanding and tracking your API costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing (as of late 2024, check current pricing!)\n",
    "# gpt-4o-mini: ~$0.15 per 1M input tokens, ~$0.60 per 1M output tokens\n",
    "\n",
    "def estimate_cost(response, model=\"gpt-4o-mini\"):\n",
    "    \"\"\"Estimate the cost of an API call\"\"\"\n",
    "    # Current pricing (verify at platform.openai.com/pricing)\n",
    "    pricing = {\n",
    "        \"gpt-4o-mini\": {\"input\": 0.15, \"output\": 0.60},  # per 1M tokens\n",
    "        \"gpt-4o\": {\"input\": 2.50, \"output\": 10.00}\n",
    "    }\n",
    "    \n",
    "    input_cost = (response.usage.prompt_tokens / 1_000_000) * pricing[model][\"input\"]\n",
    "    output_cost = (response.usage.completion_tokens / 1_000_000) * pricing[model][\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": response.usage.prompt_tokens,\n",
    "        \"output_tokens\": response.usage.completion_tokens,\n",
    "        \"total_tokens\": response.usage.total_tokens,\n",
    "        \"cost_usd\": total_cost,\n",
    "        \"cost_cents\": total_cost * 100\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a haiku about programming.\"}]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "cost_info = estimate_cost(response)\n",
    "print(f\"\\nTokens used: {cost_info['total_tokens']}\")\n",
    "print(f\"Estimated cost: ${cost_info['cost_usd']:.6f} (or {cost_info['cost_cents']:.4f} cents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **LLMs predict tokens** based on probability, they don't \"know\" facts\n",
    "2. **API structure** is simple: model + messages + parameters\n",
    "3. **Temperature** controls randomness (0 = deterministic, higher = creative)\n",
    "4. **max_tokens** limits response length and controls costs\n",
    "5. **System messages** shape the assistant's behavior\n",
    "6. **Always monitor costs** - even small calls add up!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Week 2, we'll learn how to:\n",
    "- Maintain conversation history\n",
    "- Build multi-turn conversations\n",
    "- Manage context effectively\n",
    "\n",
    "Complete the assignment to practice these concepts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
