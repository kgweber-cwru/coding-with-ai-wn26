{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kgweber-cwru/coding-with-ai-wn26/blob/main/week-1-llm-basics-and-api/concepts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Understanding LLMs and API Basics\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- Understand how large language models work at a conceptual level\n",
    "- Successfully make API calls to Vertex AI\n",
    "- Understand key API parameters and their effects\n",
    "- Build simple text generation scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: How LLMs Work (Conceptual Overview)\n",
    "\n",
    "### Tokens: The Building Blocks\n",
    "- LLMs don't see words, they see **tokens**\n",
    "- A token is roughly 3-4 characters or about 0.75 words\n",
    "- \"Hello world\" ≈ 2-3 tokens\n",
    "- This matters for cost and context limits!\n",
    "\n",
    "### Prediction and Probability\n",
    "- LLMs predict the next token based on all previous tokens\n",
    "- They assign probabilities to many possible next tokens\n",
    "- They don't \"know\" things - they predict statistically likely continuations\n",
    "- Temperature controls randomness in selection\n",
    "\n",
    "### Key Limitations\n",
    "- No real-time information (knowledge cutoff dates)\n",
    "- Can \"hallucinate\" plausible-sounding but false information\n",
    "- Cannot count tokens or characters perfectly\n",
    "- Context window limits (how much text they can \"remember\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting Up Your Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q google-genai google-auth python-dotenv\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    try:\n",
    "        PROJECT_ID = input(\"Enter your Google Cloud Project ID (press Enter to use default ADC): \").strip()\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"\"\n",
    "    if PROJECT_ID:\n",
    "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "else:\n",
    "    def find_service_account_json(max_up=6):\n",
    "        p = Path.cwd()\n",
    "        for _ in range(max_up):\n",
    "            candidate = p / \"series-2-coding-llms\" / \"creds\"\n",
    "            if candidate.exists():\n",
    "                for f in candidate.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            candidate2 = p / \"creds\"\n",
    "            if candidate2.exists():\n",
    "                for f in candidate2.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            p = p.parent\n",
    "        return None\n",
    "\n",
    "    sa_path = find_service_account_json()\n",
    "    if sa_path:\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "    else:\n",
    "        try:\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "        except Exception:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project: coding-with-ai-wn-26\n",
      "✓ Environment loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import google.auth\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "print(f\"Using project: {project}\")\n",
    "\n",
    "print(\"✓ Environment loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Your First API Call\n",
    "\n",
    "The basic structure of a Vertex AI API call:\n",
    "```python\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Your prompt here\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! It's nice to meet you. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Simple completion\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"Say hello!\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response Object\n",
    "\n",
    "Let's examine what the API returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full response object:\n",
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=10>\n",
      ") candidates=[Candidate(\n",
      "  avg_logprobs=-0.05227931908198765,\n",
      "  content=Content(\n",
      "    parts=[\n",
      "      Part(\n",
      "        text='2 + 2 = 4'\n",
      "      ),\n",
      "    ],\n",
      "    role='model'\n",
      "  ),\n",
      "  finish_reason=<FinishReason.STOP: 'STOP'>\n",
      ")] create_time=datetime.datetime(2026, 1, 25, 19, 15, 10, 405261, tzinfo=TzInfo(0)) model_version='gemini-2.5-flash-lite' prompt_feedback=None response_id='vmt2aY3eGOmQi-8PzJqLyAw' usage_metadata=GenerateContentResponseUsageMetadata(\n",
      "  candidates_token_count=7,\n",
      "  candidates_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=7\n",
      "    ),\n",
      "  ],\n",
      "  prompt_token_count=7,\n",
      "  prompt_tokens_details=[\n",
      "    ModalityTokenCount(\n",
      "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
      "      token_count=7\n",
      "    ),\n",
      "  ],\n",
      "  total_token_count=14,\n",
      "  traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>\n",
      ") automatic_function_calling_history=[] parsed=None\n",
      "\n",
      "==================================================\n",
      "\n",
      "Just the content:\n",
      "2 + 2 = 4\n",
      "\n",
      "==================================================\n",
      "\n",
      "Token usage:\n",
      "Prompt tokens: 7\n",
      "Completion tokens: 7\n",
      "Total tokens: 14\n"
     ]
    }
   ],
   "source": [
    "# Make another call and explore the response\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"What is 2+2?\"\n",
    ")\n",
    "\n",
    "print(\"Full response object:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Just the content:\")\n",
    "print(response.text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Token usage:\")\n",
    "# Check if usage metadata is available\n",
    "if response.usage_metadata:\n",
    "    print(f\"Prompt tokens: {response.usage_metadata.prompt_token_count}\")\n",
    "    print(f\"Completion tokens: {response.usage_metadata.candidates_token_count}\")\n",
    "    print(f\"Total tokens: {response.usage_metadata.total_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Key API Parameters\n",
    "\n",
    "### Temperature (0.0 to 2.0)\n",
    "Controls randomness:\n",
    "- **0.0**: Deterministic, always picks most likely token\n",
    "- **0.7**: Balanced (default for most uses)\n",
    "- **1.5+**: Very creative/random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.0:\n",
      "Here are a few ways to complete that sentence, depending on the emphasis you want to make:\n",
      "\n",
      "**Focusing on the creative aspect:**\n",
      "\n",
      "* The best thing about learning to code is **the ability to bring your ideas to life and build something\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Temperature 0.7:\n",
      "Here are a few ways to complete that sentence, depending on the emphasis you want to place:\n",
      "\n",
      "**Focusing on Empowerment & Creation:**\n",
      "\n",
      "* The best thing about learning to code is **the power it gives you to build and create anything you\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Temperature 1.5:\n",
      "Here are a few ways to complete the sentence, each highlighting a different aspect of the \"best thing\" about learning to code:\n",
      "\n",
      "**Focusing on creation and empowerment:**\n",
      "\n",
      "* The best thing about learning to code is **the power to bring\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's compare different temperatures\n",
    "prompt = \"Complete this sentence: The best thing about learning to code is\"\n",
    "\n",
    "for temp in [0.0, 0.7, 1.5]:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=temp,\n",
    "            max_output_tokens=50\n",
    "        )\n",
    "    )\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(response.text)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Tokens\n",
    "Limits the length of the response. Important for cost control!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max tokens: 20\n",
      "Let's break down what a **Large Language Model (LLM)** is in an understandable way\n",
      "Actual tokens used: 20\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Max tokens: 50\n",
      "Let's break down what a **Large Language Model (LLM)** is in an understandable way.\n",
      "\n",
      "Imagine a super-powered digital brain that's been trained on an absolutely massive amount of text and code from the internet. That's\n",
      "Actual tokens used: 50\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Max tokens: 150\n",
      "A **Large Language Model (LLM)** is a type of artificial intelligence (AI) program designed to understand, generate, and process human language. Think of it as a highly sophisticated and massive computer program that has learned the intricacies of language through exposure to vast amounts of text and code.\n",
      "\n",
      "Here's a breakdown of what makes them \"large\" and what they do:\n",
      "\n",
      "**1. \"Large\" - The Scale of Data and Parameters:**\n",
      "\n",
      "*   **Massive Datasets:** LLMs are trained on truly enormous datasets. This includes:\n",
      "    *   **Webpages:** A significant portion of the public internet.\n",
      "    *   **Books:** Digital libraries of countless books across genres.\n",
      "    *   **Articles:**\n",
      "Actual tokens used: 150\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare different max_output_tokens\n",
    "prompt = \"Explain what a large language model is.\"\n",
    "\n",
    "for max_tok in [20, 50, 150]:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            max_output_tokens=max_tok\n",
    "        )\n",
    "    )\n",
    "    print(f\"Max tokens: {max_tok}\")\n",
    "    print(response.text)\n",
    "    if response.usage_metadata:\n",
    "        print(f\"Actual tokens used: {response.usage_metadata.candidates_token_count}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Instructions\n",
    "Set the behavior and personality of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without system message:\n",
      "**DNA, which stands for Deoxyribonucleic Acid, is the fundamental molecule of life.** It's like a blueprint or a set of instructions that tells every living organism how to develop, survive, and reproduce.\n",
      "\n",
      "Here's a breakdown of what makes up DNA and its key functions:\n",
      "\n",
      "**1. The Structure: A Double Helix**\n",
      "\n",
      "*   **Nucleotides:** DNA is a polymer, meaning it's a long chain made of repeating units called nucleotides. Each nucleotide has three main components:\n",
      "    *   **A Phosphate Group:** A phosphorus atom bonded to oxygen atoms.\n",
      "    *   **A Deoxyribose Sugar:** A five-carbon sugar molecule.\n",
      "    *   **A Nitrogenous Base:** These are the \"letters\" of the genetic code. There are four types of nitrogenous bases in DNA:\n",
      "        *   **Adenine (A)**\n",
      "        *   **Guanine (G)**\n",
      "        *   **Cytosine (C)**\n",
      "        *   **Thymine (T)**\n",
      "\n",
      "*   **The \"Ladder\" Shape:** These nucleotides link together to form long strands. The sugar and phosphate groups alternate, forming the \"backbone\" of the DNA strand. The nitrogenous bases stick out from the backbone.\n",
      "\n",
      "*   **Base Pairing:** The two strands of DNA are held together by hydrogen bonds between the nitrogenous bases. This pairing is very specific:\n",
      "    *   **Adenine (A) always pairs with Thymine (T)**\n",
      "    *   **Guanine (G) always pairs with Cytosine (C)**\n",
      "    This A-T and G-C pairing is crucial for DNA replication and information transfer.\n",
      "\n",
      "*   **The Double Helix:** When these two base-paired strands twist around each other, they form the iconic **double helix** shape, resembling a twisted ladder.\n",
      "\n",
      "**2. The Function: The Genetic Code**\n",
      "\n",
      "*   **Carrying Information:** The sequence of these nitrogenous bases (A, T, C, G) along the DNA strand forms the genetic code. This code is read in \"codons,\" which are groups of three bases. Each codon specifies a particular amino acid.\n",
      "\n",
      "*   **Proteins are the Workers:** Amino acids are the building blocks of proteins. Proteins are the workhorses of the cell, carrying out a vast array of functions, including:\n",
      "    *   Building and repairing tissues\n",
      "    *   Catalyzing chemical reactions (enzymes)\n",
      "    *   Transporting molecules\n",
      "    *   Fighting off infections (antibodies)\n",
      "    *   Regulating gene expression\n",
      "\n",
      "*   **Genes:** Specific segments of DNA that code for a particular protein or functional RNA molecule are called **genes**. Humans have tens of thousands of genes.\n",
      "\n",
      "**3. Key Roles of DNA**\n",
      "\n",
      "*   **Heredity:** DNA is passed down from parents to offspring. This is how traits are inherited. When an organism reproduces, it replicates its DNA and passes a copy to its descendants.\n",
      "\n",
      "*   **Replication:** DNA has the remarkable ability to make exact copies of itself. This process, called DNA replication, ensures that each new cell receives a complete set of genetic instructions.\n",
      "\n",
      "*   **Protein Synthesis:** DNA provides the instructions for building all the proteins an organism needs. This process involves two main steps:\n",
      "    *   **Transcription:** A segment of DNA is copied into a messenger RNA (mRNA) molecule.\n",
      "    *   **Translation:** The mRNA molecule is used as a template to assemble amino acids into a protein.\n",
      "\n",
      "**In summary, DNA is a complex molecule that stores the genetic information necessary for the development, functioning, growth, and reproduction of all known living organisms.** It's the foundation of life as we know it.\n",
      "\n",
      "==================================================\n",
      "\n",
      "With system message (10-year-old level):\n",
      "Imagine your body is like a giant instruction manual. This manual tells your body *everything* it needs to know to grow, to look the way it does, and to work properly.\n",
      "\n",
      "**DNA is like the letters and words in that instruction manual!**\n",
      "\n",
      "It's a super special molecule, like a long, twisted ladder, that carries all the instructions for making you *you*.\n",
      "\n",
      "Think about it this way:\n",
      "\n",
      "*   **Every living thing has DNA:** From the tiniest ant to the biggest elephant, and even plants and tiny germs, we all have our own instruction manuals.\n",
      "*   **Your DNA is unique:** It's like your own special fingerprint! It's what makes you different from your friends and family, like why you might have curly hair or your best friend has straight hair.\n",
      "*   **It's passed down from your parents:** You get a little bit of your mom's instruction manual and a little bit of your dad's. That's why you might have your mom's eyes or your dad's smile!\n",
      "*   **DNA tells your body what to do:** It tells your cells to make things like the color of your eyes, how tall you'll be, and even how your tummy digests your yummy lunch!\n",
      "\n",
      "So, in short, **DNA is the secret code that tells your body how to be you!** It's like a tiny, amazing blueprint for life. Pretty cool, right?\n"
     ]
    }
   ],
   "source": [
    "# Without system message\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"What is DNA?\"\n",
    ")\n",
    "print(\"Without system message:\")\n",
    "print(response.text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# With system instruction\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"What is DNA?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"You are a biology teacher explaining concepts to 10-year-olds. Use simple language and fun analogies.\"\n",
    "    )\n",
    ")\n",
    "print(\"With system message (10-year-old level):\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Practical Examples\n",
    "\n",
    "### Example 1: Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Large language models are AI systems trained on massive text datasets that use neural networks to process numerical representations of words and generate human-like text for various tasks.\n"
     ]
    }
   ],
   "source": [
    "long_text = \"\"\"\n",
    "Large language models are artificial intelligence systems trained on vast amounts of text data. \n",
    "They learn patterns in language by predicting the next word in a sequence. These models have billions \n",
    "of parameters and can generate human-like text, answer questions, write code, and perform various \n",
    "language tasks. They work by converting text into numerical representations called tokens, processing \n",
    "these tokens through neural network layers, and generating probability distributions for likely next tokens.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=long_text,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"Summarize the following text in one sentence.\",\n",
    "        temperature=0.3\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I love this new feature!\n",
      "Sentiment: Positive\n",
      "\n",
      "Text: This is the worst experience ever.\n",
      "Sentiment: Negative\n",
      "\n",
      "Text: The product arrived on time.\n",
      "Sentiment: Positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def classify_sentiment(text):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=text,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=\"Classify the sentiment of the text as: positive, negative, or neutral. Respond with only one word.\",\n",
    "            temperature=0,\n",
    "            max_output_tokens=10\n",
    "        )\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "# Test it\n",
    "test_texts = [\n",
    "    \"I love this new feature!\",\n",
    "    \"This is the worst experience ever.\",\n",
    "    \"The product arrived on time.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    sentiment = classify_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted information:\n",
      "Name: John Smith | Email: john.smith@email.com | Phone: 555-123-4567\n"
     ]
    }
   ],
   "source": [
    "def extract_info(text):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=text,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=\"Extract the person's name, email, and phone number from the text. Format as: Name: X | Email: Y | Phone: Z\",\n",
    "            temperature=0\n",
    "        )\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "contact_text = \"Hi, I'm John Smith. You can reach me at john.smith@email.com or call me at 555-123-4567.\"\n",
    "\n",
    "extracted = extract_info(contact_text)\n",
    "print(\"Extracted information:\")\n",
    "print(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Cost Awareness\n",
    "\n",
    "Understanding and tracking your API costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code flows like a stream,\n",
      "Logic weaves a tapestry,\n",
      "World is built with lines.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Tokens used: 26\n",
      "Estimated cost: $0.00000830 (or 0.000830 cents)\n"
     ]
    }
   ],
   "source": [
    "# Pricing \n",
    "# Gemini 2.5 Flash Lite ~$0.10 per 1M input tokens, 0.40 per 1M output\n",
    "\n",
    "def estimate_cost(response, model=\"gemini-2.5-flash-lite\"):\n",
    "    \"\"\"Estimate the cost of an API call\"\"\"\n",
    "    # Example pricing (verify at cloud.google.com/vertex-ai/pricing)\n",
    "    pricing = {\n",
    "        \"gemini-2.5-flash-lite\": {\"input\": 0.10, \"output\": 0.40}, \n",
    "        \"gemini-2.5-pro\": {\"input\": 1.25, \"output\": 10}\n",
    "    }\n",
    "    \n",
    "    # Handle unknown models or default\n",
    "    if model not in pricing:\n",
    "        model = \"gemini-2.5-flash-lite\"\n",
    "        \n",
    "    if not response.usage_metadata:\n",
    "        return {\"cost_usd\": 0, \"total_tokens\": 0}\n",
    "\n",
    "    input_tokens = response.usage_metadata.prompt_token_count\n",
    "    output_tokens = response.usage_metadata.candidates_token_count\n",
    "    total_tokens = response.usage_metadata.total_token_count\n",
    "\n",
    "    input_cost = (input_tokens / 1_000_000) * pricing[model][\"input\"]\n",
    "    output_cost = (output_tokens / 1_000_000) * pricing[model][\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"cost_usd\": total_cost,\n",
    "        \"cost_cents\": total_cost * 100\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash-lite\",\n",
    "    contents=\"Write a haiku about programming.\"\n",
    ")\n",
    "\n",
    "print(response.text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "cost_info = estimate_cost(response, model=\"gemini-2.5-flash-lite\")\n",
    "print(f\"\\nTokens used: {cost_info['total_tokens']}\")\n",
    "print(f\"Estimated cost: ${cost_info['cost_usd']:.8f} (or {cost_info['cost_cents']:.6f} cents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **LLMs predict tokens** based on probability, they don't \"know\" facts\n",
    "2. **API structure** is simple: model + contents + parameters\n",
    "3. **Temperature** controls randomness (0 = deterministic, higher = creative)\n",
    "4. **max_output_tokens** limits response length and controls costs\n",
    "5. **System instructions** shape the model's behavior\n",
    "6. **Always monitor costs** - even small calls add up!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Week 2, we'll learn how to:\n",
    "- Maintain conversation history\n",
    "- Build multi-turn conversations\n",
    "- Manage context effectively\n",
    "\n",
    "Complete the assignment to practice these concepts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
