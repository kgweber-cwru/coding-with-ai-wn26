{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install -q google-genai google-auth python-dotenv\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    try:\n",
    "        PROJECT_ID = input(\"Enter your Google Cloud Project ID (press Enter to use default ADC): \").strip()\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"\"\n",
    "    if PROJECT_ID:\n",
    "        import os\n",
    "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "\n",
    "import os\n",
    "import google.auth\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "print(f\"Using project: {project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kgweber-cwru/coding-with-ai-wn26/blob/main/series-2-coding-llms/week-1-llm-basics-and-api/concepts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Understanding LLMs and API Basics\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- Understand how large language models work at a conceptual level\n",
    "- Successfully make API calls to Vertex AI\n",
    "- Understand key API parameters and their effects\n",
    "- Build simple text generation scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: How LLMs Work (Conceptual Overview)\n",
    "\n",
    "### Tokens: The Building Blocks\n",
    "- LLMs don't see words, they see **tokens**\n",
    "- A token is roughly 3-4 characters or about 0.75 words\n",
    "- \"Hello world\" ≈ 2-3 tokens\n",
    "- This matters for cost and context limits!\n",
    "\n",
    "### Prediction and Probability\n",
    "- LLMs predict the next token based on all previous tokens\n",
    "- They assign probabilities to many possible next tokens\n",
    "- They don't \"know\" things - they predict statistically likely continuations\n",
    "- Temperature controls randomness in selection\n",
    "\n",
    "### Key Limitations\n",
    "- No real-time information (knowledge cutoff dates)\n",
    "- Can \"hallucinate\" plausible-sounding but false information\n",
    "- Cannot count tokens or characters perfectly\n",
    "- Context window limits (how much text they can \"remember\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting Up Your Environment\n",
    "\n",
    "### Load Environment Variables\n",
    "We use `python-dotenv` to keep API keys secure and separate from code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import google.auth\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Vertex AI client\n",
    "creds, project = google.auth.default()\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "\n",
    "print(\"✓ Environment loaded successfully!\")\n",
    "print(f\"✓ Project ID found: {project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Your First API Call\n",
    "\n",
    "The basic structure of a Vertex AI API call:\n",
    "```python\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    contents=\"Your prompt here\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple completion\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    contents=\"Say hello!\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response Object\n",
    "\n",
    "Let's examine what the API returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make another call and explore the response\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    contents=\"What is 2+2?\"\n",
    ")\n",
    "\n",
    "print(\"Full response object:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Just the content:\")\n",
    "print(response.text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Token usage:\")\n",
    "# Check if usage metadata is available\n",
    "if response.usage_metadata:\n",
    "    print(f\"Prompt tokens: {response.usage_metadata.prompt_token_count}\")\n",
    "    print(f\"Completion tokens: {response.usage_metadata.candidates_token_count}\")\n",
    "    print(f\"Total tokens: {response.usage_metadata.total_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Key API Parameters\n",
    "\n",
    "### Temperature (0.0 to 2.0)\n",
    "Controls randomness:\n",
    "- **0.0**: Deterministic, always picks most likely token\n",
    "- **0.7**: Balanced (default for most uses)\n",
    "- **1.5+**: Very creative/random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare different temperatures\n",
    "prompt = \"Complete this sentence: The best thing about learning to code is\"\n",
    "\n",
    "for temp in [0.0, 0.7, 1.5]:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            temperature=temp,\n",
    "            max_output_tokens=50\n",
    "        )\n",
    "    )\n",
    "    print(f\"Temperature {temp}:\")\n",
    "    print(response.text)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Tokens\n",
    "Limits the length of the response. Important for cost control!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different max_output_tokens\n",
    "prompt = \"Explain what a large language model is.\"\n",
    "\n",
    "for max_tok in [20, 50, 150]:\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            max_output_tokens=max_tok\n",
    "        )\n",
    "    )\n",
    "    print(f\"Max tokens: {max_tok}\")\n",
    "    print(response.text)\n",
    "    if response.usage_metadata:\n",
    "        print(f\"Actual tokens used: {response.usage_metadata.candidates_token_count}\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Instructions\n",
    "Set the behavior and personality of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without system message\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    contents=\"What is DNA?\"\n",
    ")\n",
    "print(\"Without system message:\")\n",
    "print(response.text)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# With system instruction\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    contents=\"What is DNA?\",\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"You are a biology teacher explaining concepts to 10-year-olds. Use simple language and fun analogies.\"\n",
    "    )\n",
    ")\n",
    "print(\"With system message (10-year-old level):\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Practical Examples\n",
    "\n",
    "### Example 1: Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"\"\"\n",
    "Large language models are artificial intelligence systems trained on vast amounts of text data. \n",
    "They learn patterns in language by predicting the next word in a sequence. These models have billions \n",
    "of parameters and can generate human-like text, answer questions, write code, and perform various \n",
    "language tasks. They work by converting text into numerical representations called tokens, processing \n",
    "these tokens through neural network layers, and generating probability distributions for likely next tokens.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    contents=long_text,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=\"Summarize the following text in one sentence.\",\n",
    "        temperature=0.3\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(text):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=text,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=\"Classify the sentiment of the text as: positive, negative, or neutral. Respond with only one word.\",\n",
    "            temperature=0,\n",
    "            max_output_tokens=10\n",
    "        )\n",
    "    )\n",
    "    return response.text.strip()\n",
    "\n",
    "# Test it\n",
    "test_texts = [\n",
    "    \"I love this new feature!\",\n",
    "    \"This is the worst experience ever.\",\n",
    "    \"The product arrived on time.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    sentiment = classify_sentiment(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {sentiment}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(text):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=text,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=\"Extract the person's name, email, and phone number from the text. Format as: Name: X | Email: Y | Phone: Z\",\n",
    "            temperature=0\n",
    "        )\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "contact_text = \"Hi, I'm John Smith. You can reach me at john.smith@email.com or call me at 555-123-4567.\"\n",
    "\n",
    "extracted = extract_info(contact_text)\n",
    "print(\"Extracted information:\")\n",
    "print(extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Cost Awareness\n",
    "\n",
    "Understanding and tracking your API costs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pricing (Example pricing for Gemini 1.5 Flash, check official pricing!)\n",
    "# Gemini 1.5 Flash: ~$0.075 per 1M input tokens, ~$0.30 per 1M output tokens (approximate)\n",
    "\n",
    "def estimate_cost(response, model=\"gemini-1.5-flash\"):\n",
    "    \"\"\"Estimate the cost of an API call\"\"\"\n",
    "    # Example pricing (verify at cloud.google.com/vertex-ai/pricing)\n",
    "    pricing = {\n",
    "        \"gemini-1.5-flash\": {\"input\": 0.075, \"output\": 0.30}, \n",
    "        \"gemini-1.5-pro\": {\"input\": 3.50, \"output\": 10.50}\n",
    "    }\n",
    "    \n",
    "    # Handle unknown models or default\n",
    "    if model not in pricing:\n",
    "        model = \"gemini-1.5-flash\"\n",
    "        \n",
    "    if not response.usage_metadata:\n",
    "        return {\"cost_usd\": 0, \"total_tokens\": 0}\n",
    "\n",
    "    input_tokens = response.usage_metadata.prompt_token_count\n",
    "    output_tokens = response.usage_metadata.candidates_token_count\n",
    "    total_tokens = response.usage_metadata.total_token_count\n",
    "\n",
    "    input_cost = (input_tokens / 1_000_000) * pricing[model][\"input\"]\n",
    "    output_cost = (output_tokens / 1_000_000) * pricing[model][\"output\"]\n",
    "    total_cost = input_cost + output_cost\n",
    "    \n",
    "    return {\n",
    "        \"input_tokens\": input_tokens,\n",
    "        \"output_tokens\": output_tokens,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"cost_usd\": total_cost,\n",
    "        \"cost_cents\": total_cost * 100\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    contents=\"Write a haiku about programming.\"\n",
    ")\n",
    "\n",
    "print(response.text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "cost_info = estimate_cost(response, model=\"gemini-1.5-flash\")\n",
    "print(f\"\\nTokens used: {cost_info['total_tokens']}\")\n",
    "print(f\"Estimated cost: ${cost_info['cost_usd']:.8f} (or {cost_info['cost_cents']:.6f} cents)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **LLMs predict tokens** based on probability, they don't \"know\" facts\n",
    "2. **API structure** is simple: model + contents + parameters\n",
    "3. **Temperature** controls randomness (0 = deterministic, higher = creative)\n",
    "4. **max_output_tokens** limits response length and controls costs\n",
    "5. **System instructions** shape the model's behavior\n",
    "6. **Always monitor costs** - even small calls add up!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "In Week 2, we'll learn how to:\n",
    "- Maintain conversation history\n",
    "- Build multi-turn conversations\n",
    "- Manage context effectively\n",
    "\n",
    "Complete the assignment to practice these concepts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
