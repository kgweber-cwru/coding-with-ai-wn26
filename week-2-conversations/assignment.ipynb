{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install -q google-genai google-auth python-dotenv\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    try:\n",
    "        PROJECT_ID = input(\"Enter your Google Cloud Project ID (press Enter to use default ADC): \").strip()\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"\"\n",
    "    if PROJECT_ID:\n",
    "        import os\n",
    "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "\n",
    "import os\n",
    "import google.auth\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "print(f\"Using project: {project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kgweber-cwru/coding-with-ai-wn26/blob/main/series-2-coding-llms/week-2-conversations/assignment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 Assignment: Build a Conversational Application\n",
    "\n",
    "## Objective\n",
    "Create a conversational application that maintains context across multiple turns and serves a specific purpose in your domain.\n",
    "\n",
    "## Requirements\n",
    "1. Use the Conversation class (or extend it)\n",
    "2. Include a well-crafted system message\n",
    "3. Demonstrate at least 5 conversation turns\n",
    "4. Show how context is maintained across turns\n",
    "5. Track token usage and estimate costs\n",
    "\n",
    "## Ideas\n",
    "- **Research advisor**: Helps refine research questions through dialogue\n",
    "- **Study buddy**: Explains concepts and answers follow-up questions\n",
    "- **Interview bot**: Conducts structured interviews for qualitative research\n",
    "- **Writing coach**: Provides feedback and suggestions iteratively\n",
    "- **Clinical decision support**: Helps work through diagnostic reasoning\n",
    "- **Data analysis helper**: Assists with data interpretation questions\n",
    "- **Grant writing assistant**: Helps develop and refine grant proposals\n",
    "- **Literature review helper**: Discusses papers and synthesizes themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import google.auth\n",
    "\n",
    "load_dotenv()\n",
    "creds, project = google.auth.default()\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the Conversation Class\n",
    "Use the class from the concepts notebook (or extend it!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"A simple conversation manager\"\"\"\n",
    "    \n",
    "    def __init__(self, system_message=\"You are a helpful assistant.\", model=\"gemini-1.5-flash\"):\n",
    "        self.system_message = system_message\n",
    "        self.messages = [] # History of user/model turns\n",
    "        self.model = model\n",
    "        self.total_tokens = 0\n",
    "    \n",
    "    def add_user_message(self, content):\n",
    "        \"\"\"Add a user message to the conversation\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": content})\n",
    "    \n",
    "    def get_response(self, temperature=0.7, max_tokens=None):\n",
    "        \"\"\"Get assistant response and add to history\"\"\"\n",
    "        # Convert internal message format to Vertex AI Content objects\n",
    "        content_list = [\n",
    "            types.Content(role=m[\"role\"], parts=[types.Part.from_text(m[\"content\"])]) \n",
    "            for m in self.messages\n",
    "        ]\n",
    "\n",
    "        config = types.GenerateContentConfig(\n",
    "            system_instruction=self.system_message,\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=content_list,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        model_message = response.text\n",
    "        self.messages.append({\"role\": \"model\", \"content\": model_message})\n",
    "        \n",
    "        if response.usage_metadata:\n",
    "            self.total_tokens += response.usage_metadata.total_token_count\n",
    "        \n",
    "        return model_message\n",
    "    \n",
    "    def chat(self, user_message, temperature=0.7, max_tokens=None):\n",
    "        \"\"\"Convenience method: add user message and get response\"\"\"\n",
    "        self.add_user_message(user_message)\n",
    "        return self.get_response(temperature, max_tokens)\n",
    "    \n",
    "    def display_history(self):\n",
    "        \"\"\"Display the conversation history\"\"\"\n",
    "        print(f\"SYSTEM: {self.system_message}\")\n",
    "        print(\"-\" * 50)\n",
    "        for msg in self.messages:\n",
    "            role = msg[\"role\"].upper()\n",
    "            content = msg[\"content\"]\n",
    "            print(f\"{role}: {content}\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    def get_token_count(self):\n",
    "        \"\"\"Get total tokens used\"\"\"\n",
    "        return self.total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Application\n",
    "\n",
    "### Step 1: Describe Your Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR DESCRIPTION HERE**\n",
    "\n",
    "I'm building: [describe your conversational application]\n",
    "\n",
    "Purpose: [what problem does it solve?]\n",
    "\n",
    "Target users: [who would use this?]\n",
    "\n",
    "How it works: [brief explanation of the conversation flow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Design Your System Message\n",
    "\n",
    "Carefully craft a system message that defines:\n",
    "- The assistant's role\n",
    "- Its expertise area\n",
    "- How it should respond\n",
    "- Any specific behaviors or constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR SYSTEM MESSAGE HERE\n",
    "system_message = \"\"\"\n",
    "YOUR WELL-CRAFTED SYSTEM MESSAGE\n",
    "\"\"\"\n",
    "\n",
    "# Create your conversation\n",
    "my_assistant = Conversation(system_message=system_message)\n",
    "\n",
    "print(\"✓ Assistant created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Demonstrate Your Application\n",
    "\n",
    "Show a realistic conversation with at least 5 turns. Each turn should:\n",
    "- Build on previous context\n",
    "- Demonstrate the assistant's capabilities\n",
    "- Show value to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 1\n",
    "print(\"USER: [your message]\")\n",
    "print(\"\\nASSISTANT:\", my_assistant.chat(\"YOUR MESSAGE HERE\"))\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 2\n",
    "print(\"USER: [your follow-up message]\")\n",
    "print(\"\\nASSISTANT:\", my_assistant.chat(\"YOUR FOLLOW-UP HERE\"))\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 3\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 4\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn 5\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Analyze the Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display full conversation history\n",
    "print(\"FULL CONVERSATION HISTORY:\")\n",
    "print(\"=\"*70)\n",
    "my_assistant.display_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token usage and cost\n",
    "total_tokens = my_assistant.get_token_count()\n",
    "# Rough estimate for Gemini 1.5 Flash (blended rate, approx $0.15 per 1M tokens)\n",
    "estimated_cost = (total_tokens / 1_000_000) * 0.15  \n",
    "\n",
    "print(f\"Total tokens used: {total_tokens}\")\n",
    "print(f\"Estimated cost: ${estimated_cost:.6f}\")\n",
    "print(f\"Average tokens per turn: {total_tokens / 5:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection Questions\n",
    "\n",
    "### 1. How did the system message shape the conversation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Show an example where context from an earlier turn influenced a later response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**\n",
    "\n",
    "Example: In turn [X], I mentioned [Y], and in turn [Z], the assistant referred back to this by [describe how]..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What would happen if this conversation continued for 20 more turns?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**\n",
    "\n",
    "Consider: token costs, context window limits, potential drift from original purpose..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How could you improve this application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER HERE**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Challenges (Optional)\n",
    "\n",
    "### Challenge 1: Extend the Conversation Class\n",
    "Add features like:\n",
    "- Saving/loading conversations to file\n",
    "- Automatic summarization when history gets long\n",
    "- Conversation branching\n",
    "- Export to different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Compare Temperature Settings\n",
    "Run the same conversation with different temperatures (0.0, 0.7, 1.2) and compare responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Build a Multi-Persona System\n",
    "Create a system that can switch between different expert personas within a single conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Checklist\n",
    "\n",
    "- [ ] Described your application and its purpose\n",
    "- [ ] Crafted a thoughtful system message\n",
    "- [ ] Demonstrated at least 5 conversation turns\n",
    "- [ ] Showed how context is maintained\n",
    "- [ ] Analyzed token usage and costs\n",
    "- [ ] Answered all reflection questions\n",
    "- [ ] Tested with realistic examples\n",
    "- [ ] Saved your notebook!\n",
    "\n",
    "## Next Week Preview\n",
    "\n",
    "Next week: **Programmatic Prompt Engineering**\n",
    "- Dynamic prompt generation\n",
    "- Template systems\n",
    "- Few-shot learning\n",
    "- Output parsing and validation\n",
    "\n",
    "Start thinking about tasks where you need to generate similar prompts with different data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
