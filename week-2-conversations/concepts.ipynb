{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "if IN_COLAB:\n",
    "    !pip install -q google-genai google-auth python-dotenv\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    try:\n",
    "        PROJECT_ID = input(\"Enter your Google Cloud Project ID (press Enter to use default ADC): \").strip()\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"\"\n",
    "    if PROJECT_ID:\n",
    "        import os\n",
    "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "\n",
    "import os\n",
    "import google.auth\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "print(f\"Using project: {project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kgweber-cwru/coding-with-ai-wn26/blob/main/week-2-conversations/concepts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2: Building Conversations\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this session, you will:\n",
    "- Understand how conversation history works\n",
    "- Build multi-turn conversations that maintain context\n",
    "- Use system prompts effectively to shape behavior\n",
    "- Manage conversation length and costs\n",
    "- Handle different roles (system, user, assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import google.auth\n",
    "\n",
    "load_dotenv()\n",
    "creds, project = google.auth.default()\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "\n",
    "print(\"✓ Ready to build conversations!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Conversation Structure\n",
    "\n",
    "### The Messages List\n",
    "Conversations are lists of messages exchanged between the user and the model. \n",
    "- **System Instruction**: Sets the behavior/persona (passed separately in configuration)\n",
    "- **User**: The human input\n",
    "- **Model**: The AI response\n",
    "\n",
    "We can track history in a simple list:\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"model\", \"content\": \"Hi! How can I help?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Python.\"}\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Simple Two-Turn Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System instruction handles the persona\n",
    "system_instruction = \"You are a helpful teaching assistant.\"\n",
    "\n",
    "# Start with first user message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is a variable in programming?\"}\n",
    "]\n",
    "\n",
    "# Get first response\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[types.Content(role=m[\"role\"], parts=[types.Part.from_text(m[\"content\"])]) for m in messages],\n",
    "    config=types.GenerateContentConfig(system_instruction=system_instruction)\n",
    ")\n",
    "\n",
    "first_answer = response.text\n",
    "print(\"Model:\", first_answer)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Add model's response to history\n",
    "messages.append({\"role\": \"model\", \"content\": first_answer})\n",
    "\n",
    "# Add follow-up question\n",
    "messages.append({\"role\": \"user\", \"content\": \"Can you give me an example in Python?\"})\n",
    "\n",
    "# Get second response - it remembers context!\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=[types.Content(role=m[\"role\"], parts=[types.Part.from_text(m[\"content\"])]) for m in messages],\n",
    "    config=types.GenerateContentConfig(system_instruction=system_instruction)\n",
    ")\n",
    "\n",
    "print(\"Model:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Building a Conversation Manager\n",
    "\n",
    "Let's create a helper class to manage conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"A simple conversation manager\"\"\"\n",
    "    \n",
    "    def __init__(self, system_message=\"You are a helpful assistant.\", model=\"gemini-2.5-flash\"):\n",
    "        self.system_message = system_message\n",
    "        self.messages = [] # History of user/model turns\n",
    "        self.model = model\n",
    "        self.total_tokens = 0\n",
    "    \n",
    "    def add_user_message(self, content):\n",
    "        \"\"\"Add a user message to the conversation\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": content})\n",
    "    \n",
    "    def get_response(self, temperature=0.7, max_tokens=None):\n",
    "        \"\"\"Get assistant response and add to history\"\"\"\n",
    "        # Convert internal message format to Vertex AI Content objects\n",
    "        content_list = [\n",
    "            types.Content(role=m[\"role\"], parts=[types.Part.from_text(m[\"content\"])]) \n",
    "            for m in self.messages\n",
    "        ]\n",
    "\n",
    "        config = types.GenerateContentConfig(\n",
    "            system_instruction=self.system_message,\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=max_tokens\n",
    "        )\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=self.model,\n",
    "            contents=content_list,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        model_message = response.text\n",
    "        self.messages.append({\"role\": \"model\", \"content\": model_message})\n",
    "        \n",
    "        if response.usage_metadata:\n",
    "            self.total_tokens += response.usage_metadata.total_token_count\n",
    "        \n",
    "        return model_message\n",
    "    \n",
    "    def chat(self, user_message, temperature=0.7, max_tokens=None):\n",
    "        \"\"\"Convenience method: add user message and get response\"\"\"\n",
    "        self.add_user_message(user_message)\n",
    "        return self.get_response(temperature, max_tokens)\n",
    "    \n",
    "    def display_history(self):\n",
    "        \"\"\"Display the conversation history\"\"\"\n",
    "        print(f\"SYSTEM: {self.system_message}\")\n",
    "        print(\"-\" * 50)\n",
    "        for msg in self.messages:\n",
    "            role = msg[\"role\"].upper()\n",
    "            content = msg[\"content\"]\n",
    "            print(f\"{role}: {content}\")\n",
    "            print(\"-\" * 50)\n",
    "    \n",
    "    def get_token_count(self):\n",
    "        \"\"\"Get total tokens used\"\"\"\n",
    "        return self.total_tokens\n",
    "\n",
    "print(\"✓ Conversation class created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Conversation Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a conversation with a specific persona\n",
    "convo = Conversation(\n",
    "    system_message=\"You are a friendly data science tutor. Keep answers concise but clear.\"\n",
    ")\n",
    "\n",
    "# Have a multi-turn conversation\n",
    "print(convo.chat(\"What's the difference between supervised and unsupervised learning?\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(convo.chat(\"Which one would I use for clustering?\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(convo.chat(\"Give me an example algorithm for that.\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(f\"Total tokens used: {convo.get_token_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Full Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convo.display_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: System Instruction Strategies\n",
    "\n",
    "The system instruction is powerful! Let's explore different personas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persona 1: Concise expert\n",
    "expert = Conversation(\n",
    "    system_message=\"You are an expert who gives concise, technical answers. Use precise terminology.\"\n",
    ")\n",
    "\n",
    "# Persona 2: Beginner-friendly teacher\n",
    "teacher = Conversation(\n",
    "    system_message=\"You are a patient teacher explaining concepts to complete beginners. Use analogies and simple language.\"\n",
    ")\n",
    "\n",
    "# Same question to both\n",
    "question = \"What is a neural network?\"\n",
    "\n",
    "print(\"EXPERT:\")\n",
    "print(expert.chat(question))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"TEACHER:\")\n",
    "print(teacher.chat(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Output with System Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request specific output format\n",
    "structured = Conversation(\n",
    "    system_message=\"\"\"You are a medical information assistant. \n",
    "    Always structure your responses as:\n",
    "    1. DEFINITION: Brief definition\n",
    "    2. KEY POINTS: 3-4 bullet points\n",
    "    3. NOTE: Important consideration or caution\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "print(structured.chat(\"What is hypertension?\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(structured.chat(\"What about hypotension?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Managing Context Window\n",
    "\n",
    "Conversations can get too long! The model has a maximum context window (tokens it can process).\n",
    "\n",
    "### Strategy 1: Keep Recent Messages Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationWithLimit(Conversation):\n",
    "    \"\"\"Conversation that keeps only recent messages\"\"\"\n",
    "    \n",
    "    def __init__(self, system_message=\"You are a helpful assistant.\", \n",
    "                 model=\"gemini-2.5-flash\", max_history=6):\n",
    "        super().__init__(system_message, model)\n",
    "        self.max_history = max_history  # Keep last N messages\n",
    "    \n",
    "    def get_response(self, temperature=0.7, max_tokens=None):\n",
    "        # Keep only last N messages\n",
    "        if len(self.messages) > self.max_history:\n",
    "            self.messages = self.messages[-self.max_history:]\n",
    "        \n",
    "        return super().get_response(temperature, max_tokens)\n",
    "\n",
    "# Test it\n",
    "limited = ConversationWithLimit(max_history=4)\n",
    "\n",
    "for i in range(6):\n",
    "    response = limited.chat(f\"This is message number {i+1}\")\n",
    "    print(f\"Turn {i+1}: {response[:50]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(f\"Messages in memory: {len(limited.messages)}\")\n",
    "print(\"\\nCurrent history:\")\n",
    "limited.display_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy 2: Summarize Old Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_conversation(messages):\n",
    "    \"\"\"Create a summary of the conversation so far\"\"\"\n",
    "    # Format the conversation text\n",
    "    convo_text = \"\\n\".join([\n",
    "        f\"{msg['role']}: {msg['content']}\" \n",
    "        for msg in messages\n",
    "    ])\n",
    "    \n",
    "    summary_prompt = f\"\"\"Summarize this conversation in 2-3 sentences, \n",
    "    preserving key facts and context:\n",
    "    \n",
    "    {convo_text}\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=summary_prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=\"You create concise conversation summaries.\",\n",
    "            temperature=0.3\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "# Test it\n",
    "print(summarize_conversation(limited.messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Practical Conversation Applications\n",
    "\n",
    "### Application 1: Q&A Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAAssistant:\n",
    "    \"\"\"Interactive Q&A assistant with memory\"\"\"\n",
    "    \n",
    "    def __init__(self, topic=\"general knowledge\"):\n",
    "        system_msg = f\"\"\"You are a knowledgeable assistant specialized in {topic}. \n",
    "        Answer questions clearly and build on previous context in the conversation.\n",
    "        If you don't know something, say so.\"\"\"\n",
    "        self.convo = Conversation(system_message=system_msg)\n",
    "        self.topic = topic\n",
    "    \n",
    "    def ask(self, question):\n",
    "        return self.convo.chat(question)\n",
    "    \n",
    "    def history(self):\n",
    "        self.convo.display_history()\n",
    "\n",
    "# Create a Python programming assistant\n",
    "python_helper = QAAssistant(topic=\"Python programming\")\n",
    "\n",
    "print(python_helper.ask(\"What are list comprehensions?\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(python_helper.ask(\"Show me an example with filtering.\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(python_helper.ask(\"How is that different from a regular for loop?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 2: Research Interview Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterviewAssistant:\n",
    "    \"\"\"Helps conduct and document research interviews\"\"\"\n",
    "    \n",
    "    def __init__(self, research_topic):\n",
    "        system_msg = f\"\"\"You are helping conduct a research interview about {research_topic}.\n",
    "        Your role is to:\n",
    "        1. Ask thoughtful follow-up questions\n",
    "        2. Clarify ambiguous statements\n",
    "        3. Probe for more details when needed\n",
    "        4. Maintain a professional, curious tone\n",
    "        \"\"\"\n",
    "        self.convo = Conversation(system_message=system_msg)\n",
    "        self.topic = research_topic\n",
    "    \n",
    "    def respond(self, interviewee_response):\n",
    "        \"\"\"Process interviewee response and ask follow-up\"\"\"\n",
    "        return self.convo.chat(interviewee_response)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get a summary of key points from the interview\"\"\"\n",
    "        return summarize_conversation(self.convo.messages)\n",
    "\n",
    "# Example usage\n",
    "interviewer = InterviewAssistant(\"patient experiences with telemedicine\")\n",
    "\n",
    "print(\"INTERVIEWER:\", interviewer.respond(\"I started using telemedicine during COVID.\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"INTERVIEWER:\", interviewer.respond(\"It was convenient but I missed the personal connection.\"))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Interview Summary:\")\n",
    "print(interviewer.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application 3: Debugging Assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugging_assistant = Conversation(\n",
    "    system_message=\"\"\"You are a debugging assistant. When users share code and errors:\n",
    "    1. Identify the likely cause\n",
    "    2. Explain why it's happening\n",
    "    3. Suggest a fix with code\n",
    "    4. Ask clarifying questions if needed\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# Simulate debugging session\n",
    "error_report = \"\"\"I'm getting a KeyError in my Python code:\n",
    "my_dict = {'name': 'Alice', 'age': 30}\n",
    "print(my_dict['city'])\n",
    "\"\"\"\n",
    "\n",
    "print(debugging_assistant.chat(error_report))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(debugging_assistant.chat(\"How can I check if a key exists before accessing it?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Cost and Performance Considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare conversation lengths\n",
    "short_convo = Conversation()\n",
    "for i in range(3):\n",
    "    short_convo.chat(f\"Question {i+1}\")\n",
    "\n",
    "long_convo = Conversation()\n",
    "for i in range(10):\n",
    "    long_convo.chat(f\"Question {i+1}\")\n",
    "\n",
    "print(f\"Short conversation (3 turns): {short_convo.get_token_count()} tokens\")\n",
    "print(f\"Long conversation (10 turns): {long_convo.get_token_count()} tokens\")\n",
    "print(f\"\\nToken growth factor: {long_convo.get_token_count() / short_convo.get_token_count():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Conversations are message lists** - Just add to the list to maintain context\n",
    "2. **System messages are powerful** - They shape the entire conversation behavior\n",
    "3. **Context grows quickly** - Each turn includes all previous messages\n",
    "4. **Manage conversation length** - Keep recent messages or summarize old ones\n",
    "5. **Structure matters** - Clear roles and formatting help the model respond appropriately\n",
    "\n",
    "## Next Week Preview\n",
    "\n",
    "Next week, we'll explore **programmatic prompt engineering**:\n",
    "- Building dynamic prompts\n",
    "- Template systems\n",
    "- Few-shot learning\n",
    "- Output parsing\n",
    "\n",
    "Complete the assignment to practice building conversational applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
