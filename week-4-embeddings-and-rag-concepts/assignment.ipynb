{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kgweber-cwru/coding-with-ai-wn26/blob/main/series-2-coding-llms/week-4-embeddings-and-rag-concepts/assignment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 Assignment: Build Your Own Document Search System\n",
    "\n",
    "## Objective\n",
    "Create a semantic search system for a collection of documents from your domain. Demonstrate retrieval quality and use it to answer questions.\n",
    "\n",
    "## Requirements\n",
    "1. Collect or create 10+ documents relevant to your work\n",
    "2. Build a document store with embeddings\n",
    "3. Implement semantic search\n",
    "4. Demonstrate search with multiple queries\n",
    "5. Build a simple RAG system that answers questions using your documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q google-genai google-auth python-dotenv numpy\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    try:\n",
    "        PROJECT_ID = input(\"Enter your Google Cloud Project ID (press Enter to use default ADC): \").strip()\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"\"\n",
    "    if PROJECT_ID:\n",
    "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "else:\n",
    "    def find_service_account_json(max_up=6):\n",
    "        p = Path.cwd()\n",
    "        for _ in range(max_up):\n",
    "            candidate = p / \"series-2-coding-llms\" / \"creds\"\n",
    "            if candidate.exists():\n",
    "                for f in candidate.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            candidate2 = p / \"creds\"\n",
    "            if candidate2.exists():\n",
    "                for f in candidate2.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            p = p.parent\n",
    "        return None\n",
    "\n",
    "    sa_path = find_service_account_json()\n",
    "    if sa_path:\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "    else:\n",
    "        try:\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import google.auth\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "print(f\"Using project: {project}\")\n",
    "\n",
    "print(\"✅ Environment loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-004\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    response = client.models.embed_content(\n",
    "        model=model,\n",
    "        contents=text\n",
    "    )\n",
    "    return response.embeddings[0].values\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    vec1, vec2 = np.array(vec1), np.array(vec2)\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "class SimpleDocumentStore:\n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_document(self, text):\n",
    "        embedding = get_embedding(text)\n",
    "        self.documents.append(text)\n",
    "        self.embeddings.append(embedding)\n",
    "    \n",
    "    def add_documents(self, texts):\n",
    "        for text in texts:\n",
    "            self.add_document(text)\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        query_embedding = get_embedding(query)\n",
    "        similarities = [cosine_similarity(query_embedding, emb) for emb in self.embeddings]\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        return [{\"document\": self.documents[i], \"similarity\": similarities[i], \"index\": i} for i in top_indices]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Describe Your Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR DESCRIPTION**\n",
    "\n",
    "Domain: [your field]\n",
    "\n",
    "Document type: [e.g., research summaries, protocols, FAQs, case studies]\n",
    "\n",
    "Why this collection: [explain relevance]\n",
    "\n",
    "Number of documents: [X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Your Document Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR DOCUMENTS HERE\n",
    "my_documents = [\n",
    "    \"Document 1 text...\",\n",
    "    \"Document 2 text...\",\n",
    "    # Add at least 10 documents\n",
    "]\n",
    "\n",
    "print(f\"Created collection with {len(my_documents)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build Your Document Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and populate store\n",
    "store = SimpleDocumentStore()\n",
    "print(\"Adding documents...\")\n",
    "store.add_documents(my_documents)\n",
    "print(f\"✅ Indexed {len(store)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple queries\n",
    "test_queries = [\n",
    "    \"YOUR QUERY 1\",\n",
    "    \"YOUR QUERY 2\",\n",
    "    \"YOUR QUERY 3\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\"*70)\n",
    "    results = store.search(query, top_k=3)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. [Score: {result['similarity']:.3f}] {result['document'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question, doc_store, top_k=2):\n",
    "    \"\"\"Answer question using RAG\"\"\"\n",
    "    # Retrieve\n",
    "    results = doc_store.search(question, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([f\"Source {i+1}: {r['document']}\" for i, r in enumerate(results)])\n",
    "    \n",
    "    # Generate\n",
    "    prompt = f\"\"\"Answer based on the context below.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0.3)\n",
    "    )\n",
    "    \n",
    "    return {\"answer\": response.text, \"sources\": results}\n",
    "\n",
    "# Test RAG\n",
    "questions = [\n",
    "    \"YOUR QUESTION 1\",\n",
    "    \"YOUR QUESTION 2\",\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(\"=\"*70)\n",
    "    result = rag_query(question, store)\n",
    "    print(f\"A: {result['answer']}\")\n",
    "    print(\"\\nSources:\")\n",
    "    for i, src in enumerate(result['sources'], 1):\n",
    "        print(f\"  {i}. {src['document'][:80]}...\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "### 1. How well did semantic search perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Did RAG answers improve with retrieved context?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What would make this system more useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: If documents are long, implement chunking\n",
    "\n",
    "Try the chunking function from concepts and rebuild your store with chunks instead of full documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BONUS CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
