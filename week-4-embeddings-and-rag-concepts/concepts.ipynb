{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kgweber-cwru/coding-with-ai-wn26/blob/main/series-2-coding-llms/week-4-embeddings-and-rag-concepts/concepts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Embeddings and RAG Concepts\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what embeddings are and how they work\n",
    "- Generate embeddings using Vertex AI's API\n",
    "- Calculate semantic similarity between texts\n",
    "- Build a simple semantic search system\n",
    "- Understand the foundation for RAG systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q google-genai google-auth python-dotenv numpy\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    try:\n",
    "        PROJECT_ID = input(\"Enter your Google Cloud Project ID (press Enter to use default ADC): \").strip()\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"\"\n",
    "    if PROJECT_ID:\n",
    "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "else:\n",
    "    def find_service_account_json(max_up=6):\n",
    "        p = Path.cwd()\n",
    "        for _ in range(max_up):\n",
    "            candidate = p / \"series-2-coding-llms\" / \"creds\"\n",
    "            if candidate.exists():\n",
    "                for f in candidate.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            candidate2 = p / \"creds\"\n",
    "            if candidate2.exists():\n",
    "                for f in candidate2.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            p = p.parent\n",
    "        return None\n",
    "\n",
    "    sa_path = find_service_account_json()\n",
    "    if sa_path:\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "    else:\n",
    "        try:\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project: coding-with-ai-wn-26\n",
      "✅ Environment loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import google.auth\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "print(f\"Using project: {project}\")\n",
    "\n",
    "print(\"✅ Environment loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What Are Embeddings?\n",
    "\n",
    "### Concept\n",
    "- Embeddings are **numerical representations** of text\n",
    "- Text → Vector of numbers (e.g., 768 dimensions for Vertex AI's `gemini-embedding-001`)\n",
    "- Semantically similar texts have similar vectors\n",
    "- Unlike keywords, embeddings capture **meaning**\n",
    "\n",
    "### Why Embeddings?\n",
    "- Traditional search: keyword matching (\"cat\" ≠ \"feline\")\n",
    "- Embeddings: semantic matching (\"cat\" ≈ \"feline\")\n",
    "- Enable finding related content even with different words\n",
    "\n",
    "### Example\n",
    "A classic example is the analogy \"king is to man as queen is to woman\". In an embedding space, this can be expressed as:\n",
    "\n",
    "`vector('king') - vector('man') + vector('woman') ≈ vector('queen')`\n",
    "\n",
    "This means if you take the vector for 'king', subtract the vector for 'man' (to remove the \"maleness\" concept), and add the vector for 'woman' (to add the \"femaleness\" concept), you should get a vector that is very close to the vector for 'queen'.\n",
    "\n",
    "\n",
    "![king-queen](images/king-queen.png)\n",
    "\n",
    "_credit: https://ai.engin.umich.edu/2018/07/23/word-embeddings-and-how-they-vary/_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating Embeddings and Measuring Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful functions\n",
    "\n",
    "# One to calculate the embeddings using Google's embedding model\n",
    "# Locally, you could use GloVE or Word2Vec for single words\n",
    "\n",
    "def get_embedding(text, model=\"gemini-embedding-001\"):\n",
    "    \"\"\"Get embedding for a piece of text\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")  # Clean text\n",
    "    response = client.models.embed_content(\n",
    "        model=model,\n",
    "        contents=text\n",
    "    )\n",
    "    return response.embeddings[0].values\n",
    "\n",
    "# We need a cosine similarity function to find the most similar vector\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    if magnitude == 0:\n",
    "        return 0\n",
    "    return dot_product / magnitude\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our king/queen example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between (king - man + woman) and queen: 0.6087\n",
      "\n",
      "The most similar word in our vocabulary to (king - man + woman) is: 'king'\n",
      "Similarities: {'king': '0.794', 'woman': '0.737', 'queen': '0.609', 'girl': '0.597', 'princess': '0.580', 'prince': '0.537', 'boy': '0.487', 'tomato': '0.470', 'car': '0.467', 'man': '0.305'}\n"
     ]
    }
   ],
   "source": [
    "# Get embeddings for the words\n",
    "emb_king = np.array(get_embedding(\"king\"))\n",
    "emb_man = np.array(get_embedding(\"man\"))\n",
    "emb_woman = np.array(get_embedding(\"woman\"))\n",
    "emb_queen = np.array(get_embedding(\"queen\"))\n",
    "\n",
    "# Perform the vector arithmetic\n",
    "result_vec = emb_king - emb_man + emb_woman\n",
    "\n",
    "# Let's see how close our result is to 'queen'\n",
    "similarity = cosine_similarity(result_vec, emb_queen)\n",
    "print(f\"Similarity between (king - man + woman) and queen: {similarity:.4f}\")\n",
    "\n",
    "# To make this more robust, we could find the most similar word from a vocabulary\n",
    "# For this demo, we'll use a small list of candidate words.\n",
    "vocabulary = [\"queen\", \"prince\", \"princess\", \"king\", \"man\", \"woman\", \"girl\", \"boy\", \"tomato\", \"car\"]\n",
    "vocab_embeddings = {word: np.array(get_embedding(word)) for word in vocabulary}\n",
    "\n",
    "# Find the word in our vocab with the highest similarity to our result vector\n",
    "similarities = {\n",
    "    word: cosine_similarity(result_vec, vocab_emb)\n",
    "    for word, vocab_emb in vocab_embeddings.items()\n",
    "}\n",
    "\n",
    "# Find the most similar word\n",
    "most_similar_word = max(similarities, key=similarities.get)\n",
    "\n",
    "print(f\"\\nThe most similar word in our vocabulary to (king - man + woman) is: '{most_similar_word}'\")\n",
    "print(\"Similarities:\", {word: f\"{sim:.3f}\" for word, sim in sorted(similarities.items(), key=lambda item: item[1], reverse=True)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "You might notice that the result isn't perfectly 'queen'. In our case, 'king' might even show up as the most similar. Why is that?\n",
    "\n",
    "- **Model Optimization**: The classic `king - man + woman ≈ queen` example works perfectly with models like `word2vec` or `GloVe`, which were trained specifically to be good at these word-level analogies.\n",
    "- **Modern Models**: Newer models like Vertex AI's `gemini-embedding-001` are optimized for understanding the semantic meaning of **phrases and sentences**, not just single words. The simple vector arithmetic doesn't always hold up as cleanly for individual words.\n",
    "\n",
    "**However, the key takeaway is still valid.** Notice that 'queen' has a very high similarity score, much higher than unrelated words like 'boy' or 'girl', let alone 'tomato.' The similarity score for 'queen' is also significantly higher than 'man' (which we subtracted). This still demonstrates that the embedding space understands the semantic relationships between these concepts, even if the single-word arithmetic isn't perfect. It captures the \"royal\" and \"female\" concepts correctly.\n",
    "\n",
    "### Let's work with longer text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Machine learning is transforming healthcare.\n",
      "Embedding dimensions: 768\n",
      "First 10 values: [0.050925444811582565, -0.0007302068406715989, 0.01596071571111679, -0.016186965629458427, -0.05082755535840988, 0.050884198397397995, -0.0360821932554245, 0.005750035401433706, 0.000686395273078233, -0.001051548053510487]\n",
      "Embedding type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get an embedding\n",
    "text = \"Machine learning is transforming healthcare.\"\n",
    "embedding = get_embedding(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "print(f\"Embedding type: {type(embedding)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'The patient has a fever.' and 'The person is running a temperature.':\n",
      "  0.7732\n",
      "\n",
      "Similarity between 'The patient has a fever.' and 'I enjoy playing basketball.':\n",
      "  0.2746\n"
     ]
    }
   ],
   "source": [
    "# Compare similar texts\n",
    "text1 = \"The patient has a fever.\"\n",
    "text2 = \"The person is running a temperature.\"\n",
    "text3 = \"I enjoy playing basketball.\"\n",
    "\n",
    "emb1 = get_embedding(text1)\n",
    "emb2 = get_embedding(text2)\n",
    "emb3 = get_embedding(text3)\n",
    "\n",
    "print(f\"Similarity between '{text1}' and '{text2}':\")\n",
    "print(f\"  {cosine_similarity(emb1, emb2):.4f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Similarity between '{text1}' and '{text3}':\")\n",
    "print(f\"  {cosine_similarity(emb1, emb3):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Building a Document Store\n",
    "\n",
    "We will build a temporary vector-based document store: we'll store the text and embeddings for each document and make it possible to search the store as well.\n",
    "\n",
    "This is an ephemeral document store: It will evaporate as soon as you stop this runtime. But we'll look at more permanent ones next week. For the time being, it's helpful to look directly under the hood and see what it's doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Document store class created\n"
     ]
    }
   ],
   "source": [
    "class SimpleDocumentStore:\n",
    "    \"\"\"Store documents with their embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_document(self, text):\n",
    "        \"\"\"Add a document and compute its embedding\"\"\"\n",
    "        embedding = get_embedding(text)\n",
    "        self.documents.append(text)\n",
    "        self.embeddings.append(embedding)\n",
    "        return len(self.documents) - 1  # Return index\n",
    "    \n",
    "    def add_documents(self, texts):\n",
    "        \"\"\"Add multiple documents\"\"\"\n",
    "        for text in texts:\n",
    "            self.add_document(text)\n",
    "        print(f\"Added {len(texts)} documents\")\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Search for most similar documents\"\"\"\n",
    "        query_embedding = get_embedding(query)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = [\n",
    "            cosine_similarity(query_embedding, doc_emb)\n",
    "            for doc_emb in self.embeddings\n",
    "        ]\n",
    "        \n",
    "        # Get top k indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        # Return results\n",
    "        results = [\n",
    "            {\n",
    "                \"document\": self.documents[i],\n",
    "                \"similarity\": similarities[i],\n",
    "                \"index\": i\n",
    "            }\n",
    "            for i in top_indices\n",
    "        ]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "print(\"✅ Document store class created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Document Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 8 documents\n",
      "\n",
      "Store contains 8 documents\n"
     ]
    }
   ],
   "source": [
    "# Create a document store\n",
    "store = SimpleDocumentStore()\n",
    "\n",
    "# Add medical knowledge documents\n",
    "medical_docs = [\n",
    "    \"Hypertension is high blood pressure that can lead to heart disease.\",\n",
    "    \"Diabetes is a condition where blood sugar levels are too high.\",\n",
    "    \"Asthma is a respiratory condition causing breathing difficulties.\",\n",
    "    \"Migraine is a neurological condition causing severe headaches.\",\n",
    "    \"Arthritis causes inflammation and pain in the joints.\",\n",
    "    \"Pneumonia is a lung infection causing cough and fever.\",\n",
    "    \"Depression is a mental health disorder affecting mood.\",\n",
    "    \"Eczema is a skin condition causing itchy, inflamed patches.\"\n",
    "]\n",
    "\n",
    "store.add_documents(medical_docs)\n",
    "print(f\"\\nStore contains {len(store)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search the Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: What conditions affect the heart?\n",
      "======================================================================\n",
      "1. [Similarity: 0.529]\n",
      "   Hypertension is high blood pressure that can lead to heart disease.\n",
      "2. [Similarity: 0.456]\n",
      "   Asthma is a respiratory condition causing breathing difficulties.\n",
      "3. [Similarity: 0.433]\n",
      "   Depression is a mental health disorder affecting mood.\n",
      "\n",
      "Query: Tell me about breathing problems\n",
      "======================================================================\n",
      "1. [Similarity: 0.634]\n",
      "   Asthma is a respiratory condition causing breathing difficulties.\n",
      "2. [Similarity: 0.431]\n",
      "   Pneumonia is a lung infection causing cough and fever.\n",
      "3. [Similarity: 0.351]\n",
      "   Diabetes is a condition where blood sugar levels are too high.\n",
      "\n",
      "Query: Mental health issues\n",
      "======================================================================\n",
      "1. [Similarity: 0.549]\n",
      "   Depression is a mental health disorder affecting mood.\n",
      "2. [Similarity: 0.377]\n",
      "   Migraine is a neurological condition causing severe headaches.\n",
      "3. [Similarity: 0.344]\n",
      "   Eczema is a skin condition causing itchy, inflamed patches.\n"
     ]
    }
   ],
   "source": [
    "# Search with different queries\n",
    "queries = [\n",
    "    \"What conditions affect the heart?\",\n",
    "    \"Tell me about breathing problems\",\n",
    "    \"Mental health issues\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = store.search(query, top_k=3)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. [Similarity: {result['similarity']:.3f}]\")\n",
    "        print(f\"   {result['document']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Understanding RAG\n",
    "\n",
    "### What is RAG?\n",
    "**Retrieval-Augmented Generation** combines:\n",
    "1. **Retrieval**: Find relevant documents using embeddings\n",
    "2. **Augmentation**: Add retrieved docs to the prompt\n",
    "3. **Generation**: LLM generates answer using the context\n",
    "\n",
    "### Why RAG?\n",
    "- Gives LLM access to specific knowledge\n",
    "- Reduces hallucinations\n",
    "- Works with private/recent data\n",
    "- More cost-effective than fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Simple RAG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What should I know about conditions that affect breathing?\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Answer: Based on the provided documents, you should know that:\n",
      "\n",
      "*   **Asthma** is a respiratory condition that causes breathing difficulties.\n",
      "*   **Pneumonia** is a lung infection that causes cough and fever.\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Sources used:\n",
      "1. Asthma is a respiratory condition causing breathing difficulties.\n",
      "2. Pneumonia is a lung infection causing cough and fever.\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Prompt sent to model:\n",
      "Answer the question based on the context below.\n",
      "\n",
      "Context:\n",
      "Document 1: Asthma is a respiratory condition causing breathing difficulties.\n",
      "\n",
      "Document 2: Pneumonia is a lung infection causing cough and fever.\n",
      "\n",
      "Question: What should I know about conditions that affect breathing?\n",
      "\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "def simple_rag(query, document_store, top_k=2, debug=False):\n",
    "    \"\"\"Simple RAG: retrieve documents and generate answer\"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve relevant documents\n",
    "    results = document_store.search(query, top_k=top_k)\n",
    "    \n",
    "    # Step 2: Build context from retrieved documents\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}: {result['document']}\"\n",
    "        for i, result in enumerate(results)\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Create prompt with context\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 4: Generate answer\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0.3)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"answer\": response.text,\n",
    "        \"sources\": results,\n",
    "        \"prompt\": prompt if debug else \"use debug mode to view prompt\"\n",
    "    }\n",
    "\n",
    "# Test RAG\n",
    "question = \"What should I know about conditions that affect breathing?\"\n",
    "result = simple_rag(question, store, debug=True)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(\"Sources used:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"{i}. {source['document']}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(\"Prompt sent to model:\")\n",
    "print(result['prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Document Chunking\n",
    "\n",
    "For longer documents, we need to split them into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text (70 words):\n",
      "Machine learning in healthcare has revolutionized diagnostic processes. \n",
      "Computer vision models can now analyze medical images with accuracy comparable to expert radiologists. \n",
      "Natural language processing helps extract insights from clinical notes and research papers. \n",
      "Predictive models identify patients at risk of developing certain conditions. \n",
      "However, challenges remain in model interpretability, data privacy, and clinical integration. \n",
      "The future of AI in healthcare depends on collaboration between clinicians, data scientists, and patients.\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Created 5 chunks:\n",
      "\n",
      "Chunk 1:\n",
      "Machine learning in healthcare has revolutionized diagnostic processes. Computer vision models can now analyze medical images with accuracy comparable to\n",
      "\n",
      "Chunk 2:\n",
      "images with accuracy comparable to expert radiologists. Natural language processing helps extract insights from clinical notes and research papers. Predictive\n",
      "\n",
      "Chunk 3:\n",
      "notes and research papers. Predictive models identify patients at risk of developing certain conditions. However, challenges remain in model interpretability,\n",
      "\n",
      "Chunk 4:\n",
      "challenges remain in model interpretability, data privacy, and clinical integration. The future of AI in healthcare depends on collaboration between\n",
      "\n",
      "Chunk 5:\n",
      "healthcare depends on collaboration between clinicians, data scientists, and patients.\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk:  # Don't add empty chunks\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test chunking\n",
    "long_text = \"\"\"Machine learning in healthcare has revolutionized diagnostic processes. \n",
    "Computer vision models can now analyze medical images with accuracy comparable to expert radiologists. \n",
    "Natural language processing helps extract insights from clinical notes and research papers. \n",
    "Predictive models identify patients at risk of developing certain conditions. \n",
    "However, challenges remain in model interpretability, data privacy, and clinical integration. \n",
    "The future of AI in healthcare depends on collaboration between clinicians, data scientists, and patients.\"\"\"\n",
    "\n",
    "chunks = chunk_text(long_text, chunk_size=20, overlap=5)\n",
    "\n",
    "print(f\"Original text ({len(long_text.split())} words):\")\n",
    "print(long_text)\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(f\"Created {len(chunks)} chunks:\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Practical Example - Research Paper Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 4 research papers\n"
     ]
    }
   ],
   "source": [
    "# Simulate research paper abstracts\n",
    "papers = [\n",
    "    {\n",
    "        \"title\": \"Deep Learning for Medical Image Analysis\",\n",
    "        \"abstract\": \"This paper reviews convolutional neural networks for analyzing CT scans and MRI images. We demonstrate state-of-the-art performance in tumor detection.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Natural Language Processing in Clinical Documentation\",\n",
    "        \"abstract\": \"We present a transformer-based model for extracting structured information from electronic health records. Our approach reduces manual data entry time.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Predictive Models for Patient Outcomes\",\n",
    "        \"abstract\": \"Machine learning models predict hospital readmission risk using demographic and clinical data. Results show 85% accuracy in identifying high-risk patients.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Genomic Data Analysis with AI\",\n",
    "        \"abstract\": \"Deep learning identifies genetic variants associated with disease susceptibility. Our model processes whole-genome sequencing data efficiently.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create store with paper abstracts\n",
    "paper_store = SimpleDocumentStore()\n",
    "\n",
    "for paper in papers:\n",
    "    # Combine title and abstract for better searchability\n",
    "    doc_text = f\"{paper['title']}. {paper['abstract']}\"\n",
    "    paper_store.add_document(doc_text)\n",
    "\n",
    "print(f\"Indexed {len(paper_store)} research papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What papers discuss using AI for analyzing patient records?\n",
      "\n",
      "1. Similarity: 0.678\n",
      "   Natural Language Processing in Clinical Documentation. We present a transformer-based model for extracting structured information from electronic health records. Our approach reduces manual data entry time.\n",
      "\n",
      "2. Similarity: 0.597\n",
      "   Deep Learning for Medical Image Analysis. This paper reviews convolutional neural networks for analyzing CT scans and MRI images. We demonstrate state-of-the-art performance in tumor detection.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Search for papers\n",
    "research_query = \"What papers discuss using AI for analyzing patient records?\"\n",
    "\n",
    "print(f\"Query: {research_query}\\n\")\n",
    "results = paper_store.search(research_query, top_k=2)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Similarity: {result['similarity']:.3f}\")\n",
    "    print(f\"   {result['document']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Embeddings capture meaning** - Similar concepts have similar vectors\n",
    "2. **Cosine similarity measures relevance** - Higher values = more similar\n",
    "3. **Semantic search beats keywords** - Finds related content with different words\n",
    "4. **RAG = Retrieve + Generate** - Give LLMs relevant context\n",
    "5. **Chunk long documents** - Break into searchable pieces\n",
    "\n",
    "## Next Week\n",
    "\n",
    "We'll build complete RAG systems:\n",
    "- Processing document collections\n",
    "- Advanced retrieval strategies\n",
    "- Combining multiple sources\n",
    "- Production-ready patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
