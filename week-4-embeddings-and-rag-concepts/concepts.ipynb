{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q google-genai google-auth python-dotenv numpy\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    try:\n",
    "        PROJECT_ID = input(\"Enter your Google Cloud Project ID (press Enter to use default ADC): \").strip()\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"\"\n",
    "    if PROJECT_ID:\n",
    "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "else:\n",
    "    def find_service_account_json(max_up=6):\n",
    "        p = Path.cwd()\n",
    "        for _ in range(max_up):\n",
    "            candidate = p / \"series-2-coding-llms\" / \"creds\"\n",
    "            if candidate.exists():\n",
    "                for f in candidate.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            candidate2 = p / \"creds\"\n",
    "            if candidate2.exists():\n",
    "                for f in candidate2.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            p = p.parent\n",
    "        return None\n",
    "\n",
    "    sa_path = find_service_account_json()\n",
    "    if sa_path:\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "    else:\n",
    "        try:\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "import google.auth\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "print(f\"Using project: {project}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kgweber-cwru/coding-with-ai-wn26/blob/main/series-2-coding-llms/week-4-embeddings-and-rag-concepts/concepts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Embeddings and RAG Concepts\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand what embeddings are and how they work\n",
    "- Generate embeddings using Vertex AI's API\n",
    "- Calculate semantic similarity between texts\n",
    "- Build a simple semantic search system\n",
    "- Understand the foundation for RAG systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import google.auth\n",
    "\n",
    "load_dotenv('../.env')\n",
    "creds, project = google.auth.default()\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "\n",
    "print(\"✓ Ready to explore embeddings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: What Are Embeddings?\n",
    "\n",
    "### Concept\n",
    "- Embeddings are **numerical representations** of text\n",
    "- Text → Vector of numbers (e.g., 768 dimensions for Vertex AI's text-embedding-004)\n",
    "- Semantically similar texts have similar vectors\n",
    "- Unlike keywords, embeddings capture **meaning**\n",
    "\n",
    "### Why Embeddings?\n",
    "- Traditional search: keyword matching (\"cat\" ≠ \"feline\")\n",
    "- Embeddings: semantic matching (\"cat\" ≈ \"feline\")\n",
    "- Enable finding related content even with different words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-004\"):\n",
    "    \"\"\"Get embedding for a piece of text\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")  # Clean text\n",
    "    response = client.models.embed_content(\n",
    "        model=model,\n",
    "        contents=text\n",
    "    )\n",
    "    return response.embeddings[0].values\n",
    "\n",
    "# Get an embedding\n",
    "text = \"Machine learning is transforming healthcare.\"\n",
    "embedding = get_embedding(text)\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Embedding dimensions: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")\n",
    "print(f\"Embedding type: {type(embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Measuring Similarity\n",
    "\n",
    "### Cosine Similarity\n",
    "Measures how similar two vectors are (range: -1 to 1)\n",
    "- 1 = identical\n",
    "- 0 = unrelated\n",
    "- -1 = opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "    vec1 = np.array(vec1)\n",
    "    vec2 = np.array(vec2)\n",
    "    \n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    magnitude = np.linalg.norm(vec1) * np.linalg.norm(vec2)\n",
    "    \n",
    "    return dot_product / magnitude\n",
    "\n",
    "# Compare similar texts\n",
    "text1 = \"The patient has a fever.\"\n",
    "text2 = \"The person is running a temperature.\"\n",
    "text3 = \"I enjoy playing basketball.\"\n",
    "\n",
    "emb1 = get_embedding(text1)\n",
    "emb2 = get_embedding(text2)\n",
    "emb3 = get_embedding(text3)\n",
    "\n",
    "print(f\"Similarity between '{text1}' and '{text2}':\")\n",
    "print(f\"  {cosine_similarity(emb1, emb2):.4f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Similarity between '{text1}' and '{text3}':\")\n",
    "print(f\"  {cosine_similarity(emb1, emb3):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building a Document Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDocumentStore:\n",
    "    \"\"\"Store documents with their embeddings\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "    \n",
    "    def add_document(self, text):\n",
    "        \"\"\"Add a document and compute its embedding\"\"\"\n",
    "        embedding = get_embedding(text)\n",
    "        self.documents.append(text)\n",
    "        self.embeddings.append(embedding)\n",
    "        return len(self.documents) - 1  # Return index\n",
    "    \n",
    "    def add_documents(self, texts):\n",
    "        \"\"\"Add multiple documents\"\"\"\n",
    "        for text in texts:\n",
    "            self.add_document(text)\n",
    "        print(f\"Added {len(texts)} documents\")\n",
    "    \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Search for most similar documents\"\"\"\n",
    "        query_embedding = get_embedding(query)\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = [\n",
    "            cosine_similarity(query_embedding, doc_emb)\n",
    "            for doc_emb in self.embeddings\n",
    "        ]\n",
    "        \n",
    "        # Get top k indices\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        # Return results\n",
    "        results = [\n",
    "            {\n",
    "                \"document\": self.documents[i],\n",
    "                \"similarity\": similarities[i],\n",
    "                \"index\": i\n",
    "            }\n",
    "            for i in top_indices\n",
    "        ]\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.documents)\n",
    "\n",
    "print(\"✓ Document store class created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Document Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a document store\n",
    "store = SimpleDocumentStore()\n",
    "\n",
    "# Add medical knowledge documents\n",
    "medical_docs = [\n",
    "    \"Hypertension is high blood pressure that can lead to heart disease.\",\n",
    "    \"Diabetes is a condition where blood sugar levels are too high.\",\n",
    "    \"Asthma is a respiratory condition causing breathing difficulties.\",\n",
    "    \"Migraine is a neurological condition causing severe headaches.\",\n",
    "    \"Arthritis causes inflammation and pain in the joints.\",\n",
    "    \"Pneumonia is a lung infection causing cough and fever.\",\n",
    "    \"Depression is a mental health disorder affecting mood.\",\n",
    "    \"Eczema is a skin condition causing itchy, inflamed patches.\"\n",
    "]\n",
    "\n",
    "store.add_documents(medical_docs)\n",
    "print(f\"\\nStore contains {len(store)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search the Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with different queries\n",
    "queries = [\n",
    "    \"What conditions affect the heart?\",\n",
    "    \"Tell me about breathing problems\",\n",
    "    \"Mental health issues\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    results = store.search(query, top_k=3)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. [Similarity: {result['similarity']:.3f}]\")\n",
    "        print(f\"   {result['document']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Understanding RAG\n",
    "\n",
    "### What is RAG?\n",
    "**Retrieval-Augmented Generation** combines:\n",
    "1. **Retrieval**: Find relevant documents using embeddings\n",
    "2. **Augmentation**: Add retrieved docs to the prompt\n",
    "3. **Generation**: LLM generates answer using the context\n",
    "\n",
    "### Why RAG?\n",
    "- Gives LLM access to specific knowledge\n",
    "- Reduces hallucinations\n",
    "- Works with private/recent data\n",
    "- More cost-effective than fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Simple RAG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_rag(query, document_store, top_k=2):\n",
    "    \"\"\"Simple RAG: retrieve documents and generate answer\"\"\"\n",
    "    \n",
    "    # Step 1: Retrieve relevant documents\n",
    "    results = document_store.search(query, top_k=top_k)\n",
    "    \n",
    "    # Step 2: Build context from retrieved documents\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"Document {i+1}: {result['document']}\"\n",
    "        for i, result in enumerate(results)\n",
    "    ])\n",
    "    \n",
    "    # Step 3: Create prompt with context\n",
    "    prompt = f\"\"\"Answer the question based on the context below.\n",
    "    \n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Step 4: Generate answer\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0.3)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"answer\": response.text,\n",
    "        \"sources\": results\n",
    "    }\n",
    "\n",
    "# Test RAG\n",
    "question = \"What should I know about conditions that affect breathing?\"\n",
    "result = simple_rag(question, store)\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(\"Sources used:\")\n",
    "for i, source in enumerate(result['sources'], 1):\n",
    "    print(f\"{i}. {source['document']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Document Chunking\n",
    "\n",
    "For longer documents, we need to split them into chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=200, overlap=50):\n",
    "    \"\"\"Split text into overlapping chunks\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size - overlap):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        if chunk:  # Don't add empty chunks\n",
    "            chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Test chunking\n",
    "long_text = \"\"\"Machine learning in healthcare has revolutionized diagnostic processes. \n",
    "Computer vision models can now analyze medical images with accuracy comparable to expert radiologists. \n",
    "Natural language processing helps extract insights from clinical notes and research papers. \n",
    "Predictive models identify patients at risk of developing certain conditions. \n",
    "However, challenges remain in model interpretability, data privacy, and clinical integration. \n",
    "The future of AI in healthcare depends on collaboration between clinicians, data scientists, and patients.\"\"\"\n",
    "\n",
    "chunks = chunk_text(long_text, chunk_size=20, overlap=5)\n",
    "\n",
    "print(f\"Original text ({len(long_text.split())} words):\")\n",
    "print(long_text)\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "print(f\"Created {len(chunks)} chunks:\")\n",
    "for i, chunk in enumerate(chunks, 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Practical Example - Research Paper Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate research paper abstracts\n",
    "papers = [\n",
    "    {\n",
    "        \"title\": \"Deep Learning for Medical Image Analysis\",\n",
    "        \"abstract\": \"This paper reviews convolutional neural networks for analyzing CT scans and MRI images. We demonstrate state-of-the-art performance in tumor detection.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Natural Language Processing in Clinical Documentation\",\n",
    "        \"abstract\": \"We present a transformer-based model for extracting structured information from electronic health records. Our approach reduces manual data entry time.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Predictive Models for Patient Outcomes\",\n",
    "        \"abstract\": \"Machine learning models predict hospital readmission risk using demographic and clinical data. Results show 85% accuracy in identifying high-risk patients.\"\n",
    "    },\n",
    "    {\n",
    "        \"title\": \"Genomic Data Analysis with AI\",\n",
    "        \"abstract\": \"Deep learning identifies genetic variants associated with disease susceptibility. Our model processes whole-genome sequencing data efficiently.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create store with paper abstracts\n",
    "paper_store = SimpleDocumentStore()\n",
    "\n",
    "for paper in papers:\n",
    "    # Combine title and abstract for better searchability\n",
    "    doc_text = f\"{paper['title']}. {paper['abstract']}\"\n",
    "    paper_store.add_document(doc_text)\n",
    "\n",
    "print(f\"Indexed {len(paper_store)} research papers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for papers\n",
    "research_query = \"What papers discuss using AI for analyzing patient records?\"\n",
    "\n",
    "print(f\"Query: {research_query}\\n\")\n",
    "results = paper_store.search(research_query, top_k=2)\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"{i}. Similarity: {result['similarity']:.3f}\")\n",
    "    print(f\"   {result['document']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Embeddings capture meaning** - Similar concepts have similar vectors\n",
    "2. **Cosine similarity measures relevance** - Higher values = more similar\n",
    "3. **Semantic search beats keywords** - Finds related content with different words\n",
    "4. **RAG = Retrieve + Generate** - Give LLMs relevant context\n",
    "5. **Chunk long documents** - Break into searchable pieces\n",
    "\n",
    "## Next Week\n",
    "\n",
    "We'll build complete RAG systems:\n",
    "- Processing document collections\n",
    "- Advanced retrieval strategies\n",
    "- Combining multiple sources\n",
    "- Production-ready patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
