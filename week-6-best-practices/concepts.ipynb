{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Best Practices and Production Patterns\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement robust error handling\n",
    "- Optimize costs and performance\n",
    "- Test LLM applications effectively\n",
    "- Design for production deployment\n",
    "- Consider security and privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import google.auth\n",
    "from google.api_core import exceptions\n",
    "\n",
    "load_dotenv()\n",
    "creds, project = google.auth.default()\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_api_call(contents, max_retries=3, **kwargs):\n",
    "    \"\"\"Make API call with retry logic\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=kwargs.get('model', 'gemini-1.5-flash'),\n",
    "                contents=contents,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=kwargs.get('temperature', 0.7),\n",
    "                    max_output_tokens=kwargs.get('max_output_tokens', None)\n",
    "                )\n",
    "            )\n",
    "            return response.text\n",
    "            \n",
    "        except exceptions.ResourceExhausted:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff\n",
    "                print(f\"Rate limit hit. Waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return \"Error: Rate limit exceeded. Please try again later.\"\n",
    "                \n",
    "        except exceptions.GoogleAPICallError as e:\n",
    "            print(f\"API error: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                return f\"Error: API unavailable - {str(e)}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    return \"Error: Max retries exceeded\"\n",
    "\n",
    "# Test it\n",
    "result = robust_api_call(\"Say hello!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cost Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CostTracker:\n",
    "    \"\"\"Track API costs across calls\"\"\"\n",
    "    \n",
    "    PRICING = {\n",
    "        \"gemini-1.5-flash\": {\"input\": 0.075, \"output\": 0.30},  # per 1M tokens approx\n",
    "        \"gemini-1.5-pro\": {\"input\": 3.50, \"output\": 10.50},\n",
    "        \"text-embedding-004\": {\"input\": 0.00, \"output\": 0} # Often free or very cheap\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.calls = []\n",
    "        self.total_cost = 0.0\n",
    "    \n",
    "    def track_completion(self, response, model=\"gemini-1.5-flash\"):\n",
    "        \"\"\"Track a completion call\"\"\"\n",
    "        if not response.usage_metadata:\n",
    "             return 0.0\n",
    "\n",
    "        input_tokens = response.usage_metadata.prompt_token_count\n",
    "        output_tokens = response.usage_metadata.candidates_token_count\n",
    "        \n",
    "        # Default pricing if model not found\n",
    "        pricing = self.PRICING.get(model, self.PRICING[\"gemini-1.5-flash\"])\n",
    "\n",
    "        cost = (\n",
    "            (input_tokens / 1_000_000) * pricing[\"input\"] +\n",
    "            (output_tokens / 1_000_000) * pricing[\"output\"]\n",
    "        )\n",
    "        \n",
    "        self.calls.append({\n",
    "            \"model\": model,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"cost\": cost\n",
    "        })\n",
    "        \n",
    "        self.total_cost += cost\n",
    "        return cost\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Generate cost report\"\"\"\n",
    "        if not self.calls:\n",
    "            return \"No calls tracked\"\n",
    "        \n",
    "        total_input = sum(c['input_tokens'] for c in self.calls)\n",
    "        total_output = sum(c['output_tokens'] for c in self.calls)\n",
    "        \n",
    "        return f\"\"\"Cost Report:\n",
    "Total calls: {len(self.calls)}\n",
    "Total input tokens: {total_input:,}\n",
    "Total output tokens: {total_output:,}\n",
    "Total cost: ${self.total_cost:.8f}\n",
    "Average cost per call: ${self.total_cost/len(self.calls):.8f}\"\"\"\n",
    "\n",
    "# Test it\n",
    "tracker = CostTracker()\n",
    "\n",
    "for i in range(3):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-1.5-flash\",\n",
    "        contents=f\"Count to {i+1}\"\n",
    "    )\n",
    "    cost = tracker.track_completion(response)\n",
    "    print(f\"Call {i+1} cost: ${cost:.8f}\")\n",
    "\n",
    "print(\"\\n\" + tracker.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Testing Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMTestSuite:\n",
    "    \"\"\"Test suite for LLM applications\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tests = []\n",
    "        self.results = []\n",
    "    \n",
    "    def add_test(self, name, prompt, expected_contains=None, expected_not_contains=None):\n",
    "        \"\"\"Add a test case\"\"\"\n",
    "        self.tests.append({\n",
    "            \"name\": name,\n",
    "            \"prompt\": prompt,\n",
    "            \"expected_contains\": expected_contains or [],\n",
    "            \"expected_not_contains\": expected_not_contains or []\n",
    "        })\n",
    "    \n",
    "    def run_tests(self, temperature=0):\n",
    "        \"\"\"Run all tests\"\"\"\n",
    "        print(f\"Running {len(self.tests)} tests...\\n\")\n",
    "        \n",
    "        for test in self.tests:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-1.5-flash\",\n",
    "                contents=test[\"prompt\"],\n",
    "                config=types.GenerateContentConfig(temperature=temperature)\n",
    "            )\n",
    "            \n",
    "            output = response.text.lower()\n",
    "            \n",
    "            # Check expectations\n",
    "            passed = True\n",
    "            failures = []\n",
    "            \n",
    "            for expected in test[\"expected_contains\"]:\n",
    "                if expected.lower() not in output:\n",
    "                    passed = False\n",
    "                    failures.append(f\"Missing: {expected}\")\n",
    "            \n",
    "            for not_expected in test[\"expected_not_contains\"]:\n",
    "                if not_expected.lower() in output:\n",
    "                    passed = False\n",
    "                    failures.append(f\"Should not contain: {not_expected}\")\n",
    "            \n",
    "            result = {\n",
    "                \"name\": test[\"name\"],\n",
    "                \"passed\": passed,\n",
    "                \"output\": output,\n",
    "                \"failures\": failures\n",
    "            }\n",
    "            \n",
    "            self.results.append(result)\n",
    "            \n",
    "            status = \"✓ PASS\" if passed else \"✗ FAIL\"\n",
    "            print(f\"{status}: {test['name']}\")\n",
    "            if failures:\n",
    "                for f in failures:\n",
    "                    print(f\"  - {f}\")\n",
    "        \n",
    "        # Summary\n",
    "        passed_count = sum(1 for r in self.results if r['passed'])\n",
    "        print(f\"\\n{passed_count}/{len(self.tests)} tests passed\")\n",
    "\n",
    "# Example tests\n",
    "suite = LLMTestSuite()\n",
    "\n",
    "suite.add_test(\n",
    "    \"Math calculation\",\n",
    "    \"What is 15 + 27? Reply with only the number.\",\n",
    "    expected_contains=[\"42\"]\n",
    ")\n",
    "\n",
    "suite.add_test(\n",
    "    \"Medical terminology\",\n",
    "    \"Translate 'hypertension' to plain language in one sentence.\",\n",
    "    expected_contains=[\"blood pressure\", \"high\"],\n",
    "    expected_not_contains=[\"hypertension\"]  # Should use plain language\n",
    ")\n",
    "\n",
    "suite.run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Caching for Cost Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "class CachedLLM:\n",
    "    \"\"\"LLM with response caching\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _make_key(self, contents, **kwargs):\n",
    "        \"\"\"Create cache key from request\"\"\"\n",
    "        key_dict = {\n",
    "            \"contents\": contents,\n",
    "            \"model\": kwargs.get('model', 'gemini-1.5-flash'),\n",
    "            \"temperature\": kwargs.get('temperature', 0.7)\n",
    "        }\n",
    "        # Handle string or list contents for hashing\n",
    "        key_str = json.dumps(key_dict, sort_keys=True, default=str)\n",
    "        return hashlib.md5(key_str.encode()).hexdigest()\n",
    "    \n",
    "    def complete(self, contents, **kwargs):\n",
    "        \"\"\"Complete with caching\"\"\"\n",
    "        key = self._make_key(contents, **kwargs)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            self.hits += 1\n",
    "            print(\"  [Cache hit]\")\n",
    "            return self.cache[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        print(\"  [Cache miss - calling API]\")\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=kwargs.get('model', 'gemini-1.5-flash'),\n",
    "            contents=contents,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=kwargs.get('temperature', 0.7)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        result = response.text\n",
    "        self.cache[key] = result\n",
    "        return result\n",
    "    \n",
    "    def stats(self):\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = self.hits / total if total > 0 else 0\n",
    "        return f\"Cache stats: {self.hits} hits, {self.misses} misses ({hit_rate:.1%} hit rate)\"\n",
    "\n",
    "# Test caching\n",
    "cached_llm = CachedLLM()\n",
    "\n",
    "# Same request multiple times\n",
    "for i in range(3):\n",
    "    print(f\"\\nRequest {i+1}:\")\n",
    "    result = cached_llm.complete(\n",
    "        \"What is the capital of France?\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + cached_llm.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Input Validation and Safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeLLMApp:\n",
    "    \"\"\"LLM application with input validation\"\"\"\n",
    "    \n",
    "    MAX_INPUT_LENGTH = 2000  # characters\n",
    "    MAX_TOKENS = 500\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_input(user_input):\n",
    "        \"\"\"Validate user input\"\"\"\n",
    "        if not user_input or not user_input.strip():\n",
    "            return False, \"Input cannot be empty\"\n",
    "        \n",
    "        if len(user_input) > SafeLLMApp.MAX_INPUT_LENGTH:\n",
    "            return False, f\"Input too long (max {SafeLLMApp.MAX_INPUT_LENGTH} chars)\"\n",
    "        \n",
    "        # Add other validation as needed\n",
    "        # - Check for malicious patterns\n",
    "        # - Filter PII if required\n",
    "        # - Check for prompt injection attempts\n",
    "        \n",
    "        return True, \"OK\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sanitize_output(output):\n",
    "        \"\"\"Clean up LLM output\"\"\"\n",
    "        # Remove potential PII or sensitive info\n",
    "        # Format output consistently\n",
    "        # Add disclaimers if needed\n",
    "        return output.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_query(user_input, system_message=\"You are a helpful assistant.\"):\n",
    "        \"\"\"Process query safely\"\"\"\n",
    "        # Validate\n",
    "        valid, message = SafeLLMApp.validate_input(user_input)\n",
    "        if not valid:\n",
    "            return {\"error\": message}\n",
    "        \n",
    "        try:\n",
    "            # Call API with limits\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-1.5-flash\",\n",
    "                contents=user_input,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    system_instruction=system_message,\n",
    "                    max_output_tokens=SafeLLMApp.MAX_TOKENS,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            output = response.text\n",
    "            return {\"response\": SafeLLMApp.sanitize_output(output)}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Processing failed: {str(e)}\"}\n",
    "\n",
    "# Test safe query\n",
    "result = SafeLLMApp.safe_query(\"What is machine learning?\")\n",
    "print(result)\n",
    "\n",
    "# Test validation\n",
    "result = SafeLLMApp.safe_query(\"\")  # Empty input\n",
    "print(result)\n",
    "\n",
    "result = SafeLLMApp.safe_query(\"x\" * 3000)  # Too long\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Production Checklist\n",
    "\n",
    "### Before deploying to production:\n",
    "\n",
    "#### Security\n",
    "- [ ] API keys stored securely (environment variables, secret manager)\n",
    "- [ ] Input validation implemented\n",
    "- [ ] Output sanitization for PII\n",
    "- [ ] Rate limiting on user requests\n",
    "- [ ] Prompt injection protection\n",
    "\n",
    "#### Reliability\n",
    "- [ ] Error handling for all API calls\n",
    "- [ ] Retry logic with exponential backoff\n",
    "- [ ] Timeout handling\n",
    "- [ ] Graceful degradation when API unavailable\n",
    "- [ ] Logging for debugging\n",
    "\n",
    "#### Cost Management\n",
    "- [ ] Cost tracking implemented\n",
    "- [ ] Token limits set appropriately\n",
    "- [ ] Caching for repeated queries\n",
    "- [ ] Budget alerts configured\n",
    "- [ ] Regular cost monitoring\n",
    "\n",
    "#### Quality\n",
    "- [ ] Test suite with expected behaviors\n",
    "- [ ] Edge case testing\n",
    "- [ ] Output quality evaluation\n",
    "- [ ] User feedback mechanism\n",
    "- [ ] A/B testing capability\n",
    "\n",
    "#### User Experience\n",
    "- [ ] Clear error messages\n",
    "- [ ] Loading indicators\n",
    "- [ ] Source attribution (for RAG)\n",
    "- [ ] Disclaimers where appropriate\n",
    "- [ ] Feedback collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Always handle errors** - APIs fail, networks drop, limits hit\n",
    "2. **Track costs** - They add up quickly in production\n",
    "3. **Test systematically** - LLMs are probabilistic, test thoroughly\n",
    "4. **Cache when possible** - Save money and improve speed\n",
    "5. **Validate inputs** - Protect against misuse and errors\n",
    "6. **Monitor in production** - Watch costs, errors, and quality\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the series! You now know how to:\n",
    "- Work with LLM APIs\n",
    "- Build conversations\n",
    "- Engineer prompts programmatically\n",
    "- Use embeddings for semantic search\n",
    "- Build RAG systems\n",
    "- Deploy production-ready applications\n",
    "\n",
    "### Next Steps\n",
    "- Build real applications in your domain\n",
    "- Explore advanced topics (fine-tuning, agents, etc.)\n",
    "- Share your work with colleagues\n",
    "- Keep learning as the field evolves!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
