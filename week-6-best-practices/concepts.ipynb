{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kgweber-cwru/coding-with-ai-wn26/blob/main/week-6-best-practices/concepts.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Best Practices and Production Patterns\n",
    "\n",
    "## Learning Objectives\n",
    "- Implement robust error handling\n",
    "- Optimize costs and performance\n",
    "- Test LLM applications effectively\n",
    "- Design for production deployment\n",
    "- Consider security and privacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !pip install -q google-genai google-auth google-api-core python-dotenv numpy chromadb\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    try:\n",
    "        PROJECT_ID = input(\"Enter your Google Cloud Project ID (press Enter to use default ADC): \").strip()\n",
    "    except Exception:\n",
    "        PROJECT_ID = \"\"\n",
    "    if PROJECT_ID:\n",
    "        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
    "else:\n",
    "    def find_service_account_json(max_up=6):\n",
    "        p = Path.cwd()\n",
    "        for _ in range(max_up):\n",
    "            candidate = p / \"series-2-coding-llms\" / \"creds\"\n",
    "            if candidate.exists():\n",
    "                for f in candidate.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            candidate2 = p / \"creds\"\n",
    "            if candidate2.exists():\n",
    "                for f in candidate2.glob(\"*.json\"):\n",
    "                    return str(f.resolve())\n",
    "            p = p.parent\n",
    "        return None\n",
    "\n",
    "    sa_path = find_service_account_json()\n",
    "    if sa_path:\n",
    "        os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = sa_path\n",
    "    else:\n",
    "        try:\n",
    "            from dotenv import load_dotenv\n",
    "            load_dotenv()\n",
    "        except Exception:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using project: coding-with-ai-wn-26\n",
      "✅ Environment loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import google.auth\n",
    "from google.api_core import exceptions\n",
    "\n",
    "creds, project = google.auth.default()\n",
    "project = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", project)\n",
    "client = genai.Client(vertexai=True, project=project, location=\"us-central1\")\n",
    "print(f\"Using project: {project}\")\n",
    "\n",
    "print(\"✅ Environment loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Error Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "def robust_api_call(contents, max_retries=3, **kwargs):\n",
    "    \"\"\"Make API call with retry logic\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.models.generate_content(\n",
    "                model=kwargs.get('model', 'gemini-2.5-flash-lite'),\n",
    "                contents=contents,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    temperature=kwargs.get('temperature', 0.7),\n",
    "                    max_output_tokens=kwargs.get('max_output_tokens', None)\n",
    "                )\n",
    "            )\n",
    "            return response.text\n",
    "            \n",
    "        except exceptions.ResourceExhausted as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # Exponential backoff\n",
    "                print(f\"Rate limit hit. Waiting {wait_time}s...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return \"Error: Rate limit exceeded. Please try again later.\"\n",
    "                \n",
    "        except exceptions.GoogleAPICallError as e:\n",
    "            print(f\"API error: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                return f\"Error: API unavailable - {str(e)}\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    return \"Error: Max retries exceeded\"\n",
    "\n",
    "# Test it\n",
    "result = robust_api_call(\"Say hello!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cost Management\n",
    "\n",
    "Pricing is super dynamic and even for a given model, depends on the volume and type of thinking.  Here's Google's pricelist:\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/pricing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call 1 cost: $0.00000080\n",
      "Call 2 cost: $0.00000200\n",
      "Call 3 cost: $0.00000360\n",
      "\n",
      "Cost Report:\n",
      "Total calls: 3\n",
      "Total input tokens: 12\n",
      "Total output tokens: 13\n",
      "Total cost: $0.00000640\n",
      "Average cost per call: $0.00000213\n"
     ]
    }
   ],
   "source": [
    "class CostTracker:\n",
    "    \"\"\"Track API costs across calls\"\"\"\n",
    "    \n",
    "    PRICING = {\n",
    "        \"gemini-2.5-flash-lite\": {\"input\": 0.10, \"output\": 0.40},  # per 1M tokens approx\n",
    "        \"gemini-2.5-pro\": {\"input\": 1.25, \"output\": 10.00},\n",
    "        \"gemini-embedding-001\": {\"input\": 0.15, \"output\": 0} # Often free or very cheap\n",
    "    }\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.calls = []\n",
    "        self.total_cost = 0.0\n",
    "    \n",
    "    def track_completion(self, response, model=\"gemini-2.5-flash-lite\"):\n",
    "        \"\"\"Track a completion call\"\"\"\n",
    "        if not response.usage_metadata:\n",
    "             return 0.0\n",
    "\n",
    "        input_tokens = response.usage_metadata.prompt_token_count\n",
    "        output_tokens = response.usage_metadata.candidates_token_count\n",
    "        \n",
    "        # Default pricing if model not found\n",
    "        pricing = self.PRICING.get(model, self.PRICING[\"gemini-2.5-flash-lite\"])\n",
    "\n",
    "        cost = (\n",
    "            (input_tokens / 1_000_000) * pricing[\"input\"] +\n",
    "            (output_tokens / 1_000_000) * pricing[\"output\"]\n",
    "        )\n",
    "        \n",
    "        self.calls.append({\n",
    "            \"model\": model,\n",
    "            \"input_tokens\": input_tokens,\n",
    "            \"output_tokens\": output_tokens,\n",
    "            \"cost\": cost\n",
    "        })\n",
    "        \n",
    "        self.total_cost += cost\n",
    "        return cost\n",
    "    \n",
    "    def report(self):\n",
    "        \"\"\"Generate cost report\"\"\"\n",
    "        if not self.calls:\n",
    "            return \"No calls tracked\"\n",
    "        \n",
    "        total_input = sum(c['input_tokens'] for c in self.calls)\n",
    "        total_output = sum(c['output_tokens'] for c in self.calls)\n",
    "        \n",
    "        return f\"\"\"Cost Report:\n",
    "Total calls: {len(self.calls)}\n",
    "Total input tokens: {total_input:,}\n",
    "Total output tokens: {total_output:,}\n",
    "Total cost: ${self.total_cost:.8f}\n",
    "Average cost per call: ${self.total_cost/len(self.calls):.8f}\"\"\"\n",
    "\n",
    "# Test it\n",
    "tracker = CostTracker()\n",
    "\n",
    "for i in range(3):\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-lite\",\n",
    "        contents=f\"Count to {i+1}\"\n",
    "    )\n",
    "    cost = tracker.track_completion(response)\n",
    "    print(f\"Call {i+1} cost: ${cost:.8f}\")\n",
    "\n",
    "print(\"\\n\" + tracker.report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Testing Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 2 tests...\n",
      "\n",
      "✓ PASS: Math calculation\n",
      "✗ FAIL: Medical terminology\n",
      "  - Should not contain: hypertension\n",
      "\n",
      "1/2 tests passed\n"
     ]
    }
   ],
   "source": [
    "class LLMTestSuite:\n",
    "    \"\"\"Test suite for LLM applications\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tests = []\n",
    "        self.results = []\n",
    "    \n",
    "    def add_test(self, name, prompt, expected_contains=None, expected_not_contains=None):\n",
    "        \"\"\"Add a test case\"\"\"\n",
    "        self.tests.append({\n",
    "            \"name\": name,\n",
    "            \"prompt\": prompt,\n",
    "            \"expected_contains\": expected_contains or [],\n",
    "            \"expected_not_contains\": expected_not_contains or []\n",
    "        })\n",
    "    \n",
    "    def run_tests(self, temperature=0):\n",
    "        \"\"\"Run all tests\"\"\"\n",
    "        print(f\"Running {len(self.tests)} tests...\\n\")\n",
    "        \n",
    "        for test in self.tests:\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash-lite\",\n",
    "                contents=test[\"prompt\"],\n",
    "                config=types.GenerateContentConfig(temperature=temperature)\n",
    "            )\n",
    "            \n",
    "            output = response.text.lower()\n",
    "            \n",
    "            # Check expectations\n",
    "            passed = True\n",
    "            failures = []\n",
    "            \n",
    "            for expected in test[\"expected_contains\"]:\n",
    "                if expected.lower() not in output:\n",
    "                    passed = False\n",
    "                    failures.append(f\"Missing: {expected}\")\n",
    "            \n",
    "            for not_expected in test[\"expected_not_contains\"]:\n",
    "                if not_expected.lower() in output:\n",
    "                    passed = False\n",
    "                    failures.append(f\"Should not contain: {not_expected}\")\n",
    "            \n",
    "            result = {\n",
    "                \"name\": test[\"name\"],\n",
    "                \"passed\": passed,\n",
    "                \"output\": output,\n",
    "                \"failures\": failures\n",
    "            }\n",
    "            \n",
    "            self.results.append(result)\n",
    "            \n",
    "            status = \"✓ PASS\" if passed else \"✗ FAIL\"\n",
    "            print(f\"{status}: {test['name']}\")\n",
    "            if failures:\n",
    "                for f in failures:\n",
    "                    print(f\"  - {f}\")\n",
    "        \n",
    "        # Summary\n",
    "        passed_count = sum(1 for r in self.results if r['passed'])\n",
    "        print(f\"\\n{passed_count}/{len(self.tests)} tests passed\")\n",
    "\n",
    "# Example tests\n",
    "suite = LLMTestSuite()\n",
    "\n",
    "suite.add_test(\n",
    "    \"Math calculation\",\n",
    "    \"What is 15 + 27? Reply with only the number.\",\n",
    "    expected_contains=[\"42\"]\n",
    ")\n",
    "\n",
    "suite.add_test(\n",
    "    \"Medical terminology\",\n",
    "    \"Translate 'hypertension' to plain language in one sentence.\",\n",
    "    expected_contains=[\"blood pressure\", \"high\"],\n",
    "    expected_not_contains=[\"hypertension\"]  # Should use plain language\n",
    ")\n",
    "\n",
    "suite.run_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Caching for Cost Reduction\n",
    "\n",
    "Here's a class to help you do it on your own.\n",
    "\n",
    "Additionally, though, most current LLMs have some caching you can activate.\n",
    "\n",
    "Gemini models implicitly cache (1024 tokens for `flash`, 4096 for `pro`.) You can see the number of tokens which were cache hits in the response object's `usage_metadata` field.\n",
    "\n",
    "Detailed documentation for how to use Gemini's explict caching capability is here:\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/caching?lang=python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Request 1:\n",
      "  [Cache miss - calling API]\n",
      "\n",
      "Request 2:\n",
      "  [Cache hit]\n",
      "\n",
      "Request 3:\n",
      "  [Cache hit]\n",
      "\n",
      "Cache stats: 2 hits, 1 misses (66.7% hit rate)\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import json\n",
    "\n",
    "class CachedLLM:\n",
    "    \"\"\"LLM with response caching\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _make_key(self, contents, **kwargs):\n",
    "        \"\"\"Create cache key from request\"\"\"\n",
    "        key_dict = {\n",
    "            \"contents\": contents,\n",
    "            \"model\": kwargs.get('model', 'gemini-2.5-flash-lite'),\n",
    "            \"temperature\": kwargs.get('temperature', 0.7)\n",
    "        }\n",
    "        # Handle string or list contents for hashing\n",
    "        key_str = json.dumps(key_dict, sort_keys=True, default=str)\n",
    "        return hashlib.md5(key_str.encode()).hexdigest()\n",
    "    \n",
    "    def complete(self, contents, **kwargs):\n",
    "        \"\"\"Complete with caching\"\"\"\n",
    "        key = self._make_key(contents, **kwargs)\n",
    "        \n",
    "        if key in self.cache:\n",
    "            self.hits += 1\n",
    "            print(\"  [Cache hit]\")\n",
    "            return self.cache[key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        print(\"  [Cache miss - calling API]\")\n",
    "        \n",
    "        response = client.models.generate_content(\n",
    "            model=kwargs.get('model', 'gemini-2.5-flash-lite'),\n",
    "            contents=contents,\n",
    "            config=types.GenerateContentConfig(\n",
    "                temperature=kwargs.get('temperature', 0.7)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        result = response.text\n",
    "        self.cache[key] = result\n",
    "        return result\n",
    "    \n",
    "    def stats(self):\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = self.hits / total if total > 0 else 0\n",
    "        return f\"Cache stats: {self.hits} hits, {self.misses} misses ({hit_rate:.1%} hit rate)\"\n",
    "\n",
    "# Test caching\n",
    "cached_llm = CachedLLM()\n",
    "\n",
    "# Same request multiple times\n",
    "for i in range(3):\n",
    "    print(f\"\\nRequest {i+1}:\")\n",
    "    result = cached_llm.complete(\n",
    "        \"What is the capital of France?\",\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "print(\"\\n\" + cached_llm.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Input Validation and Safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'Machine learning is a subfield of artificial intelligence (AI) that focuses on **enabling computer systems to learn from data without being explicitly programmed.** Instead of providing a computer with a rigid set of instructions to perform a task, machine learning algorithms allow the computer to identify patterns, make predictions, and improve its performance over time as it\\'s exposed to more data.\\n\\nThink of it like teaching a child. You don\\'t give them a step-by-step manual for every single thing they\\'ll encounter. Instead, you show them examples, explain concepts, and they gradually learn to understand and respond to new situations. Machine learning works in a similar way.\\n\\nHere\\'s a breakdown of the key concepts:\\n\\n**1. Data is King:**\\n* Machine learning algorithms rely heavily on **data**. This data can be anything from images, text, numbers, sensor readings, or even sounds.\\n* The quality and quantity of data significantly impact the performance of a machine learning model.\\n\\n**2. Learning from Patterns:**\\n* Algorithms analyze the data to discover **patterns, relationships, and insights**.\\n* These patterns are then used to build a **model**, which is essentially a mathematical representation of the learned knowledge.\\n\\n**3. Making Predictions or Decisions:**\\n* Once a model is trained, it can be used to make **predictions** about new, unseen data or to make **decisions** based on the learned patterns.\\n* For example, a model trained on past customer behavior could predict which products a new customer is likely to buy.\\n\\n**4. Improvement over Time:**\\n* A core aspect of machine learning is its ability to **improve its performance** as it encounters more data. This is often referred to as \"learning.\"\\n* The more data a model processes, the better it generally becomes at its task.\\n\\n**Types of Machine Learning:**\\n\\nThere are three main categories of machine learning:\\n\\n*   **Supervised Learning:**\\n    *   In this type, the algorithm is trained on **labeled data**. This means each data point has a corresponding \"correct answer\" or \"output.\"\\n    *   **Examples:**\\n        *   **Classification:** Predicting a category (e.g., spam or not spam, image of a cat or dog).\\n        *   **Regression:** Predicting a continuous value (e.g., house prices, stock market trends).\\n\\n*   **Unsupervised Learning:**'}\n",
      "{'error': 'Input cannot be empty'}\n",
      "{'error': 'Input too long (max 2000 chars)'}\n"
     ]
    }
   ],
   "source": [
    "class SafeLLMApp:\n",
    "    \"\"\"LLM application with input validation\"\"\"\n",
    "    \n",
    "    MAX_INPUT_LENGTH = 2000  # characters\n",
    "    MAX_TOKENS = 500\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_input(user_input):\n",
    "        \"\"\"Validate user input\"\"\"\n",
    "        if not user_input or not user_input.strip():\n",
    "            return False, \"Input cannot be empty\"\n",
    "        \n",
    "        if len(user_input) > SafeLLMApp.MAX_INPUT_LENGTH:\n",
    "            return False, f\"Input too long (max {SafeLLMApp.MAX_INPUT_LENGTH} chars)\"\n",
    "        \n",
    "        # Add other validation as needed\n",
    "        # - Check for malicious patterns\n",
    "        # - Filter PII if required\n",
    "        # - Check for prompt injection attempts\n",
    "        \n",
    "        return True, \"OK\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sanitize_output(output):\n",
    "        \"\"\"Clean up LLM output\"\"\"\n",
    "        # Remove potential PII or sensitive info\n",
    "        # Format output consistently\n",
    "        # Add disclaimers if needed\n",
    "        return output.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_query(user_input, system_message=\"You are a helpful assistant.\"):\n",
    "        \"\"\"Process query safely\"\"\"\n",
    "        # Validate\n",
    "        valid, message = SafeLLMApp.validate_input(user_input)\n",
    "        if not valid:\n",
    "            return {\"error\": message}\n",
    "        \n",
    "        try:\n",
    "            # Call API with limits\n",
    "            response = client.models.generate_content(\n",
    "                model=\"gemini-2.5-flash-lite\",\n",
    "                contents=user_input,\n",
    "                config=types.GenerateContentConfig(\n",
    "                    system_instruction=system_message,\n",
    "                    max_output_tokens=SafeLLMApp.MAX_TOKENS,\n",
    "                    temperature=0.7\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            output = response.text\n",
    "            return {\"response\": SafeLLMApp.sanitize_output(output)}\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"Processing failed: {str(e)}\"}\n",
    "\n",
    "# Test safe query\n",
    "result = SafeLLMApp.safe_query(\"What is machine learning?\")\n",
    "print(result)\n",
    "\n",
    "# Test validation\n",
    "result = SafeLLMApp.safe_query(\"\")  # Empty input\n",
    "print(result)\n",
    "\n",
    "result = SafeLLMApp.safe_query(\"x\" * 3000)  # Too long\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Production Checklist\n",
    "\n",
    "### Before deploying to production:\n",
    "\n",
    "#### Security\n",
    "- [ ] API keys stored securely (environment variables, secret manager)\n",
    "- [ ] Input validation implemented\n",
    "- [ ] Output sanitization for PII\n",
    "- [ ] Rate limiting on user requests\n",
    "- [ ] Prompt injection protection\n",
    "\n",
    "#### Reliability\n",
    "- [ ] Error handling for all API calls\n",
    "- [ ] Retry logic with exponential backoff\n",
    "- [ ] Timeout handling\n",
    "- [ ] Graceful degradation when API unavailable\n",
    "- [ ] Logging for debugging\n",
    "\n",
    "#### Cost Management\n",
    "- [ ] Cost tracking implemented\n",
    "- [ ] Token limits set appropriately\n",
    "- [ ] Caching for repeated queries\n",
    "- [ ] Budget alerts configured\n",
    "- [ ] Regular cost monitoring\n",
    "\n",
    "#### Quality\n",
    "- [ ] Test suite with expected behaviors\n",
    "- [ ] Edge case testing\n",
    "- [ ] Output quality evaluation\n",
    "- [ ] User feedback mechanism\n",
    "- [ ] A/B testing capability\n",
    "\n",
    "#### User Experience\n",
    "- [ ] Clear error messages\n",
    "- [ ] Loading indicators\n",
    "- [ ] Source attribution (for RAG)\n",
    "- [ ] Disclaimers where appropriate\n",
    "- [ ] Feedback collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Always handle errors** - APIs fail, networks drop, limits hit\n",
    "2. **Track costs** - They add up quickly in production\n",
    "3. **Test systematically** - LLMs are probabilistic, test thoroughly\n",
    "4. **Cache when possible** - Save money and improve speed\n",
    "5. **Validate inputs** - Protect against misuse and errors\n",
    "6. **Monitor in production** - Watch costs, errors, and quality\n",
    "\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the series! You now know how to:\n",
    "- Work with LLM APIs\n",
    "- Build conversations\n",
    "- Engineer prompts programmatically\n",
    "- Use embeddings for semantic search\n",
    "- Build RAG systems\n",
    "- Deploy production-ready applications\n",
    "\n",
    "### Next Steps\n",
    "- Build real applications in your domain\n",
    "- Explore advanced topics (fine-tuning, agents, etc.)\n",
    "- Share your work with colleagues\n",
    "- Keep learning as the field evolves!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_seminar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
